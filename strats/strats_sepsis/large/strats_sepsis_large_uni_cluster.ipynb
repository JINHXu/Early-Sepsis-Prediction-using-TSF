{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTDFCBgnk1o3"
   },
   "source": [
    "# Sepsis prediction with Strats\n",
    "\n",
    "- __lean version, run all cells immediate and once__\n",
    "\n",
    "- physiological features only, no texts\n",
    "\n",
    "- small dataset `pre_text_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKAJiLXiTlN"
   },
   "source": [
    "## Hardware check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 10 15:19:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    38W / 300W |     21MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    38W / 300W |     35MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    39W / 300W |     29MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    38W / 300W |     17MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1834      G   /usr/libexec/Xorg                  19MiB |\n",
      "|    1   N/A  N/A      1834      G   /usr/libexec/Xorg                  35MiB |\n",
      "|    2   N/A  N/A      1834      G   /usr/libexec/Xorg                  28MiB |\n",
      "|    3   N/A  N/A      1834      G   /usr/libexec/Xorg                  16MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BRXUglWE5fvf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas==1.3.4\n",
      "  Using cached pandas-1.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from pandas==1.3.4) (1.24.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from pandas==1.3.4) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to pandas in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm==4.62.3\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to tqdm in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.21.2\n",
      "  Using cached numpy-1.21.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to numpy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn==0.24.2\n",
      "  Using cached scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from scikit-learn==0.24.2) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.9.3)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to scikit-learn in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy==1.7.1\n",
      "  Using cached scipy-1.7.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
      "Collecting numpy<1.23.0,>=1.16.5\n",
      "  Using cached numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to scipy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting setuptools==58.0.4\n",
      "  Using cached setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to setuptools in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorboard==1.15.0\n",
      "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (3.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (1.16.0)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (1.51.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (0.38.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (41.6.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (1.24.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard==1.15.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==1.15.0) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard==1.15.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.15.0) (3.11.0)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to tensorboard in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.3.0\n",
      "  Using cached tensorflow-2.3.0-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.11.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.51.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.14.1)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.19.6)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.1.1)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.38.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (41.6.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.2)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to gast in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-base==1.15.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-base==1.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-estimator==2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to tensorflow-estimator in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas==1.3.4\n",
    "! pip install tqdm==4.62.3\n",
    "! pip install numpy==1.21.2\n",
    "! pip install scikit-learn==0.24.2\n",
    "! pip install scipy==1.7.1\n",
    "! pip install setuptools==58.0.4\n",
    "# # ! pip install sip==4.19.8\n",
    "# # ! pip install six==1.16.0\n",
    "# # ! pip install sqlite==3.36.0\n",
    "# ! pip install tensorboard==1.15.0\n",
    "# # ! pip install tensorflow==2.3.0\n",
    "# ! pip install tensorflow-base==1.15.0\n",
    "# ! pip install tensorflow-estimator==2.3.0\n",
    "# # ! pip install tensorflow-gpu==1.15.0\n",
    "# ! pip install tensorflow-gpu==2.3.0\n",
    "! pip install tensorboard==1.15.0\n",
    "! pip install tensorflow==2.3.0\n",
    "! pip install tensorflow-base==1.15.0\n",
    "! pip install tensorflow-estimator==2.3.0\n",
    "! pip install tensorflow-gpu==1.15.0\n",
    "# ! pip uninstall tensorflow tensorflow-gpu\n",
    "# ! pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RpTg0QzPqqKv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 13:51:08.904744: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 13:51:14.020489: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-11 13:51:31.305046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bwhpc/common/devel/cuda/11.8/lib64\n",
      "2023-03-11 13:51:31.318127: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bwhpc/common/devel/cuda/11.8/lib64\n",
      "2023-03-11 13:51:31.318142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "import smart_cond as sc\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylFHWoydK5S9"
   },
   "source": [
    "### Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AeG1PfTVsOsd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dVyQwdzHkJ_e",
    "outputId": "749008cb-6ee2-45c2-d4d8-df639cf07d59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.format_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKWDkwoeSGU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HIpIrmCQuQB",
    "outputId": "ebef1111-ee94-47af-e725-4fc4090ed7a8"
   },
   "outputs": [],
   "source": [
    "# connect to drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XA2r00BXM5pb",
    "outputId": "e82df45b-cb2e-4b90-ca82-37d0721633cf"
   },
   "outputs": [],
   "source": [
    "#! unzip /content/drive/MyDrive/sepsis/strats_exp/data/sepsis_data/pre_text/pre_text_small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LRnIP6VYSHtd"
   },
   "outputs": [],
   "source": [
    "# mortality data\n",
    "data_path = 'sepsis_data/forecasting_exp1/sepsis_pretext_large.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "# data, oc, train_ind, valid_ind, test_ind = pd.read_pickle(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zJBSNivnTV__",
    "outputId": "a4801151-8e81-42c8-f540-486936c0ee31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>TABLE</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Age</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>64.053647</td>\n",
       "      <td>56.625699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Gender</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.438951</td>\n",
       "      <td>0.496263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>DBP</td>\n",
       "      <td>-0.517967</td>\n",
       "      <td>chart</td>\n",
       "      <td>59.766756</td>\n",
       "      <td>14.994705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>GCS_eye</td>\n",
       "      <td>0.679313</td>\n",
       "      <td>chart</td>\n",
       "      <td>3.274060</td>\n",
       "      <td>1.068640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>GCS_motor</td>\n",
       "      <td>0.515191</td>\n",
       "      <td>chart</td>\n",
       "      <td>5.271144</td>\n",
       "      <td>1.414728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81478793</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>MBP</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>chart</td>\n",
       "      <td>78.552377</td>\n",
       "      <td>17.645628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81478794</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>O2 Saturation</td>\n",
       "      <td>-0.678068</td>\n",
       "      <td>chart</td>\n",
       "      <td>96.820961</td>\n",
       "      <td>4.160290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81478795</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>RR</td>\n",
       "      <td>0.179866</td>\n",
       "      <td>chart</td>\n",
       "      <td>26.278501</td>\n",
       "      <td>15.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81478796</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>SBP</td>\n",
       "      <td>-0.404061</td>\n",
       "      <td>chart</td>\n",
       "      <td>120.239648</td>\n",
       "      <td>25.341836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81478797</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>Urine</td>\n",
       "      <td>-0.242960</td>\n",
       "      <td>output</td>\n",
       "      <td>123.393012</td>\n",
       "      <td>137.442433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81478798 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_ind       hour       variable      value   TABLE        mean  \\\n",
       "0              0   0.000000            Age  66.000000     N/A   64.053647   \n",
       "1              0   0.000000         Gender   1.000000     N/A    0.438951   \n",
       "2              0   0.033333            DBP  -0.517967   chart   59.766756   \n",
       "3              0   0.033333        GCS_eye   0.679313   chart    3.274060   \n",
       "4              0   0.033333      GCS_motor   0.515191   chart    5.271144   \n",
       "...          ...        ...            ...        ...     ...         ...   \n",
       "81478793   57281  20.400000            MBP   0.195381   chart   78.552377   \n",
       "81478794   57281  20.400000  O2 Saturation  -0.678068   chart   96.820961   \n",
       "81478795   57281  20.400000             RR   0.179866   chart   26.278501   \n",
       "81478796   57281  20.400000            SBP  -0.404061   chart  120.239648   \n",
       "81478797   57281  20.400000          Urine  -0.242960  output  123.393012   \n",
       "\n",
       "                 std  \n",
       "0          56.625699  \n",
       "1           0.496263  \n",
       "2          14.994705  \n",
       "3           1.068640  \n",
       "4           1.414728  \n",
       "...              ...  \n",
       "81478793   17.645628  \n",
       "81478794    4.160290  \n",
       "81478795   15.130729  \n",
       "81478796   25.341836  \n",
       "81478797  137.442433  \n",
       "\n",
       "[81478798 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pkl[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "KmEoXh50TW5S",
    "outputId": "f5d14811-3bf8-4394-ab97-398805602bef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>in_hospital_sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>110404</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>188028</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>173727</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>164716</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>158689</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57784</th>\n",
       "      <td>45535</td>\n",
       "      <td>143774</td>\n",
       "      <td>94944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57785</th>\n",
       "      <td>45536</td>\n",
       "      <td>123750</td>\n",
       "      <td>94950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57786</th>\n",
       "      <td>50475</td>\n",
       "      <td>196881</td>\n",
       "      <td>94953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57787</th>\n",
       "      <td>45537</td>\n",
       "      <td>118475</td>\n",
       "      <td>94954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57788</th>\n",
       "      <td>45538</td>\n",
       "      <td>156386</td>\n",
       "      <td>94956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ts_ind  HADM_ID  SUBJECT_ID  in_hospital_sepsis\n",
       "0           0   110404         268                   1\n",
       "1           1   188028         270                   0\n",
       "2           2   173727         271                   0\n",
       "3           3   164716         272                   0\n",
       "4           4   158689         273                   0\n",
       "...       ...      ...         ...                 ...\n",
       "57784   45535   143774       94944                   0\n",
       "57785   45536   123750       94950                   0\n",
       "57786   50475   196881       94953                   0\n",
       "57787   45537   118475       94954                   0\n",
       "57788   45538   156386       94956                   0\n",
       "\n",
       "[57282 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = pkl[1]\n",
    "oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBQBTQ97YoKh"
   },
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s2Ia_xndajsw"
   },
   "outputs": [],
   "source": [
    "data.loc[data['variable'] == 'Antibiotics', 'value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T-7yqvARz74Y"
   },
   "outputs": [],
   "source": [
    "data.loc[data['variable'] == 'Blood Culture', 'value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e97yJy_m0AVU"
   },
   "outputs": [],
   "source": [
    "data.loc[data['variable'] == 'Mechanically ventilated', 'value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete overlapped variable due to typo in mortality data\n",
    "data = data[data['variable'] != 'vacomycin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f4AnDyVx9ZvX"
   },
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "oc = oc.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dq8P4rTtAjt"
   },
   "source": [
    "## Load Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMmN8uxppdJn"
   },
   "source": [
    "### Genrate New Patient IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "i5eNS2sep1v7"
   },
   "outputs": [],
   "source": [
    "ids = oc['SUBJECT_ID'].tolist()\n",
    "labels = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "new_patient_ids = []\n",
    "new_labels = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  # print(i)\n",
    "  if ids[i] in new_patient_ids:\n",
    "    continue\n",
    "  else:\n",
    "    new_patient_ids.append(ids[i])\n",
    "    new_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2h4TCoWrz_f",
    "outputId": "73f2c756-33a6-4ee4-8bd2-35029623bad6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 5288, 0: 51994})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# data ratio\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXaR1Yrd_arq"
   },
   "source": [
    "#### Split train/val/test \n",
    "\n",
    "_on patient level: 64/16/20_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DGsFjUwpronH"
   },
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "\n",
    " x, x_test, y, y_test = train_test_split(new_patient_ids, new_labels, test_size=0.2, random_state=1)\n",
    " x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAhA5hUdEGjZ"
   },
   "source": [
    "### Generate train_ind/val_ind/test_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qi7EHsZ-G-E9"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_ind = []\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGGyZSapL128",
    "outputId": "f5dccb15-0d70-4481-ee5d-ebe444ef8495"
   },
   "outputs": [],
   "source": [
    "test_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "    \n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVHglqHeMNA5",
    "outputId": "8d1a74f9-6e8c-4811-9ab9-d2622280d277"
   },
   "outputs": [],
   "source": [
    "valid_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxp0djnwMlfl"
   },
   "source": [
    "## Load Forecasting Data into Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FoPWLNZJMeAg"
   },
   "outputs": [],
   "source": [
    "pred_window = 2 # hours\n",
    "obs_windows = range(20, 124, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVBS63qcMyFA",
    "outputId": "7b080d9c-d003-4c12-8cda-18a5392ce8d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91626it [00:00, 798748.17it/s]\n",
      "100%|██████████| 26/26 [04:41<00:00, 10.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)    \n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLrH_WYlNjsD"
   },
   "source": [
    "## Load Target Dataset into Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zt7omOuRNtMC"
   },
   "outputs": [],
   "source": [
    "# # load data again\n",
    "# data_path = '/pfs/data5/home/hd/hd_hd/hd_ry236/riezler/data/patient/mimic_iii_preprocessed_finetuning2.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XHqsrvuP7DRd"
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "data.loc[data['variable'] == 'Mechanically ventilated', 'value'] = 1\n",
    "data.loc[data['variable'] == 'Blood Culture', 'value'] = 1\n",
    "data.loc[data['variable'] == 'Antibiotics', 'value'] = 1\n",
    "\n",
    "# delete overlapped variable due to typo in mortality data\n",
    "data = data[data['variable'] != 'vacomycin']\n",
    "\n",
    "data = data.dropna()\n",
    "oc = oc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UgeQw8nOBAO",
    "outputId": "f3f27728-0208-41ba-d091-51e4e00dac0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28576\n",
      "36551\n",
      "8931\n",
      "11469\n",
      "7145\n",
      "9262\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_ind = []\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "# ids = ids\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])\n",
    "# number of train patients\n",
    "print(len(x_train))\n",
    "# number of train instances\n",
    "print(len(train_ind))\n",
    "# to np.array\n",
    "train_ind = np.array(train_ind)\n",
    "test_ind = []\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "# number of test patients\n",
    "print(len(x_test))\n",
    "# number of test instances\n",
    "print(len(test_ind))\n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)\n",
    "valid_ind = []\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "# number of test patients\n",
    "print(len(x_val))\n",
    "# number of test instances\n",
    "print(len(valid_ind))\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6q0kcqoNajt",
    "outputId": "5f606881-ecfc-4484-dbc3-81b54fcbcaae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114564it [00:00, 855382.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18953971it [00:26, 722501.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Read data.\n",
    "# data_path = './../mimic_iii_preprocessed.pkl'\n",
    "# data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_sepsis']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "train_op = y[train_ind]\n",
    "valid_op = y[valid_ind]\n",
    "test_op = y[test_ind]\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwTr65yNxK5n"
   },
   "source": [
    "## Define metrics and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VS2qNfuNPkET"
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUWWxIUHyMus"
   },
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RN5qidLvPnxn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGjR-HJCyU1y"
   },
   "source": [
    "## Pretrain on forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBEksc7Ei-Kc",
    "outputId": "72700b50-b678-4ff2-f17e-083737bddb53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 23:00:21.524260: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 23:00:23.397488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30940 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n",
      "2023-03-09 23:00:23.398193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30926 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2023-03-09 23:00:23.398721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30933 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n",
      "2023-03-09 23:00:23.399255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30945 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 880, 50)      6700        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " cve (CVE)                      (None, 880, 50)      364         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " cve_1 (CVE)                    (None, 880, 50)      364         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 880, 50)      0           ['embedding[0][0]',              \n",
      "                                                                  'cve[0][0]',                    \n",
      "                                                                  'cve_1[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 880)          0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " transformer (Transformer)      (None, 880, 50)      39508       ['add[0][0]',                    \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 880, 1)       5200        ['transformer[0][0]',            \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          300         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 50)           0           ['transformer[0][0]',            \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           5050        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100)          0           ['lambda_1[0][0]',               \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 133)          13433       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,919\n",
      "Trainable params: 70,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3200 [00:00<?, ?it/s]2023-03-09 23:00:33.313546: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x14ef2d3540b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-09 23:00:33.313578: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-09 23:00:33.313584: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-09 23:00:33.313590: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-09 23:00:33.313593: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-09 23:00:33.350739: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-09 23:00:33.529607: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "0.279546: 100%|██████████| 3200/3200 [02:45<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 7.5977\n",
      "Epoch 0 loss 8.942761608660222 val loss 7.59768533706665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.225858: 100%|██████████| 3200/3200 [02:33<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 6.8833\n",
      "Epoch 1 loss 7.22526930399239 val loss 6.883316516876221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.210608: 100%|██████████| 3200/3200 [02:34<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 6.6462\n",
      "Epoch 2 loss 6.737427093014121 val loss 6.646219253540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.205799:  99%|█████████▉| 3171/3200 [02:31<00:01, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 6.3253\n",
      "Epoch 3 loss 6.591130437478423 val loss 6.325290203094482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.196824: 100%|██████████| 3200/3200 [02:32<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 6.1552\n",
      "Epoch 4 loss 6.296450631394983 val loss 6.155197620391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.190801: 100%|██████████| 3200/3200 [02:32<00:00, 21.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 6.2320\n",
      "Epoch 5 loss 6.103771440982818 val loss 6.232028961181641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.192007: 100%|██████████| 3200/3200 [02:31<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.9767\n",
      "Epoch 6 loss 6.142366041690111 val loss 5.976655006408691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.184503: 100%|██████████| 3200/3200 [02:33<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.8806\n",
      "Epoch 7 loss 5.902311885133385 val loss 5.8806352615356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.184397: 100%|██████████| 3200/3200 [02:32<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.9417\n",
      "Epoch 8 loss 5.89890869345516 val loss 5.9416985511779785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.182896: 100%|██████████| 3200/3200 [02:33<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.9879\n",
      "Epoch 9 loss 5.850895053595305 val loss 5.987901210784912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.180143: 100%|██████████| 3200/3200 [02:33<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.7430\n",
      "Epoch 10 loss 5.762838219814003 val loss 5.7430291175842285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.179004: 100%|██████████| 3200/3200 [02:32<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.6967\n",
      "Epoch 11 loss 5.726389525197447 val loss 5.696687698364258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.179564: 100%|██████████| 3200/3200 [02:32<00:00, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.6740\n",
      "Epoch 12 loss 5.744322857819498 val loss 5.674041748046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.177430: 100%|██████████| 3200/3200 [02:33<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.6891\n",
      "Epoch 13 loss 5.676031022034586 val loss 5.689107894897461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.175424: 100%|██████████| 3200/3200 [02:35<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 71s 17ms/step - loss: 5.6983\n",
      "Epoch 14 loss 5.611880024150014 val loss 5.698296546936035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.171579: 100%|██████████| 3200/3200 [02:33<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.5935\n",
      "Epoch 15 loss 5.488875172398984 val loss 5.593545436859131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.172094: 100%|██████████| 3200/3200 [02:33<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.5832\n",
      "Epoch 16 loss 5.505331055819989 val loss 5.583189010620117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.182157: 100%|██████████| 3200/3200 [02:33<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.6793\n",
      "Epoch 17 loss 5.827269548326731 val loss 5.679285049438477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.169903: 100%|██████████| 3200/3200 [02:32<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4520\n",
      "Epoch 18 loss 5.435248183198273 val loss 5.45196008682251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.169962: 100%|██████████| 3200/3200 [02:32<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.4822\n",
      "Epoch 19 loss 5.437123303748667 val loss 5.4821977615356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.170401: 100%|██████████| 3200/3200 [02:33<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.4834\n",
      "Epoch 20 loss 5.451167787089944 val loss 5.483358860015869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.170917: 100%|██████████| 3200/3200 [02:33<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4721\n",
      "Epoch 21 loss 5.467675392441452 val loss 5.472147464752197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165770:  24%|██▍       | 776/3200 [00:37<01:56, 20.74it/s]"
     ]
    }
   ],
   "source": [
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 102400, 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print (fore_model.summary())\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "\n",
    "# save losses for visualization\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for e in range(1000):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        e_loss += fore_model.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    val_loss = fore_model.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    val_losses.append(val_loss)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    train_losses.append(e_loss*batch_size/samples_per_epoch)\n",
    "    # model should be saved here after each epoch in case of unexpected disconnection\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        fore_model.save_weights(fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 880, 50)      6700        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " cve_2 (CVE)                    (None, 880, 50)      364         ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " cve_3 (CVE)                    (None, 880, 50)      364         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 880, 50)      0           ['embedding_1[0][0]',            \n",
      "                                                                  'cve_2[0][0]',                  \n",
      "                                                                  'cve_3[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 880)          0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " transformer_1 (Transformer)    (None, 880, 50)      39508       ['add_1[0][0]',                  \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 880, 1)       5200        ['transformer_1[0][0]',          \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          300         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 50)           0           ['transformer_1[0][0]',          \n",
      "                                                                  'attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50)           5050        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 100)          0           ['lambda_3[0][0]',               \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 133)          13433       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,919\n",
      "Trainable params: 70,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.167361: 100%|██████████| 3200/3200 [02:36<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.4925\n",
      "Epoch 0 loss 5.353945109210908 val loss 5.492489814758301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.173288: 100%|██████████| 3200/3200 [02:33<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4839\n",
      "Epoch 1 loss 5.543537951447069 val loss 5.483879566192627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.167328: 100%|██████████| 3200/3200 [02:35<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4290\n",
      "Epoch 2 loss 5.3528854877129195 val loss 5.428975582122803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.166334: 100%|██████████| 3200/3200 [02:31<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4512\n",
      "Epoch 3 loss 5.321064526587724 val loss 5.451188087463379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.168636: 100%|██████████| 3200/3200 [02:32<00:00, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.5152\n",
      "Epoch 4 loss 5.394729016311467 val loss 5.515200138092041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.166042: 100%|██████████| 3200/3200 [02:32<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4371\n",
      "Epoch 5 loss 5.311722205393016 val loss 5.437069892883301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.168234: 100%|██████████| 3200/3200 [02:32<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3653\n",
      "Epoch 6 loss 5.381858346201479 val loss 5.365274429321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.167013: 100%|██████████| 3200/3200 [02:32<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.5656\n",
      "Epoch 7 loss 5.342788154035807 val loss 5.565596103668213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.168373: 100%|██████████| 3200/3200 [02:33<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3864\n",
      "Epoch 8 loss 5.386319348998367 val loss 5.386378765106201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165177: 100%|██████████| 3200/3200 [02:33<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.4124\n",
      "Epoch 9 loss 5.2840783436596395 val loss 5.412416458129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.169693: 100%|██████████| 3200/3200 [02:32<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.4696\n",
      "Epoch 10 loss 5.428544378019869 val loss 5.469552516937256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.166113: 100%|██████████| 3200/3200 [02:33<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3375\n",
      "Epoch 11 loss 5.313990867733955 val loss 5.3375020027160645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163429: 100%|██████████| 3200/3200 [02:34<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 71s 17ms/step - loss: 5.3840\n",
      "Epoch 12 loss 5.228149870112539 val loss 5.384041786193848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.167686: 100%|██████████| 3200/3200 [02:32<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.6043\n",
      "Epoch 13 loss 5.364315086975694 val loss 5.604312896728516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.170914: 100%|██████████| 3200/3200 [02:34<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3469\n",
      "Epoch 14 loss 5.467587303742766 val loss 5.346930980682373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.169228: 100%|██████████| 3200/3200 [02:33<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3188\n",
      "Epoch 15 loss 5.4136669323220845 val loss 5.318793773651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165230: 100%|██████████| 3200/3200 [02:34<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3094\n",
      "Epoch 16 loss 5.285757878199219 val loss 5.309434413909912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165810: 100%|██████████| 3200/3200 [02:33<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3006\n",
      "Epoch 17 loss 5.30430496327579 val loss 5.30056095123291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165573: 100%|██████████| 3200/3200 [02:34<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2875\n",
      "Epoch 18 loss 5.296718424782157 val loss 5.287532329559326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.166772: 100%|██████████| 3200/3200 [02:34<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2897\n",
      "Epoch 19 loss 5.33507310859859 val loss 5.289671421051025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.161835: 100%|██████████| 3200/3200 [02:34<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.3042\n",
      "Epoch 20 loss 5.177161651663482 val loss 5.30417537689209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.164574: 100%|██████████| 3200/3200 [02:33<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3520\n",
      "Epoch 21 loss 5.264780966900289 val loss 5.351980209350586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163098: 100%|██████████| 3200/3200 [02:34<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.2475\n",
      "Epoch 22 loss 5.217542752437294 val loss 5.247509002685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.167908: 100%|██████████| 3200/3200 [02:33<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.9408\n",
      "Epoch 23 loss 5.37143972620368 val loss 5.940810680389404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163546: 100%|██████████| 3200/3200 [02:34<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.2390\n",
      "Epoch 24 loss 5.231900888048113 val loss 5.239004611968994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160422: 100%|██████████| 3200/3200 [02:33<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 17ms/step - loss: 5.2945\n",
      "Epoch 25 loss 5.131959644258022 val loss 5.294470310211182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163462: 100%|██████████| 3200/3200 [02:34<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2638\n",
      "Epoch 26 loss 5.229186567403376 val loss 5.2638044357299805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163021: 100%|██████████| 3200/3200 [02:33<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2311\n",
      "Epoch 27 loss 5.21510216768831 val loss 5.231130123138428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159546: 100%|██████████| 3200/3200 [02:33<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3319\n",
      "Epoch 28 loss 5.103939463123679 val loss 5.331860542297363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160658: 100%|██████████| 3200/3200 [02:34<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3030\n",
      "Epoch 29 loss 5.1395072543621065 val loss 5.302984237670898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.165537: 100%|██████████| 3200/3200 [02:35<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2094\n",
      "Epoch 30 loss 5.295594905279577 val loss 5.20936918258667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163428: 100%|██████████| 3200/3200 [02:34<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2793\n",
      "Epoch 31 loss 5.228117607161403 val loss 5.279294967651367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163028: 100%|██████████| 3200/3200 [02:34<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.6611\n",
      "Epoch 32 loss 5.215331567898392 val loss 5.661080360412598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163993: 100%|██████████| 3200/3200 [02:35<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2425\n",
      "Epoch 33 loss 5.246187872886658 val loss 5.242515563964844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163938: 100%|██████████| 3200/3200 [02:34<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2505\n",
      "Epoch 34 loss 5.244423152990639 val loss 5.250489234924316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160558: 100%|██████████| 3200/3200 [02:34<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2358\n",
      "Epoch 35 loss 5.1362876399606465 val loss 5.235843658447266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163767: 100%|██████████| 3200/3200 [02:35<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.1931\n",
      "Epoch 36 loss 5.238962321206928 val loss 5.193099021911621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.162719: 100%|██████████| 3200/3200 [02:34<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2302\n",
      "Epoch 37 loss 5.205428397282958 val loss 5.23015832901001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159376: 100%|██████████| 3200/3200 [02:34<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2337\n",
      "Epoch 38 loss 5.098473454117775 val loss 5.2337327003479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159422: 100%|██████████| 3200/3200 [02:35<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2268\n",
      "Epoch 39 loss 5.099950809329748 val loss 5.226778030395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.162892: 100%|██████████| 3200/3200 [02:34<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2149\n",
      "Epoch 40 loss 5.210957710072398 val loss 5.214933395385742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.163826: 100%|██████████| 3200/3200 [02:33<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2147\n",
      "Epoch 41 loss 5.240831747725606 val loss 5.214661598205566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.161351: 100%|██████████| 3200/3200 [02:33<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 17ms/step - loss: 5.2531\n",
      "Epoch 42 loss 5.161659680530429 val loss 5.253082275390625\n"
     ]
    }
   ],
   "source": [
    "# continue training after disconnected (4h runtime limit with uni-cluster, and 24h runtime limit with colab)\n",
    "\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 102400, 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print(fore_model.summary())\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "fore_model.load_weights(fore_savepath)\n",
    "\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "\n",
    "# save losses for visualization\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for e in range(1000):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        e_loss += fore_model.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    val_loss = fore_model.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    val_losses.append(val_loss)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    train_losses.append(e_loss*batch_size/samples_per_epoch)\n",
    "#     # model should be saved here after each epoch in case of unexpected disconnection\n",
    "#     files.download(model_path)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        fore_model.save_weights(fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 880)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 880, 50)      6700        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " cve_2 (CVE)                    (None, 880, 50)      364         ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " cve_3 (CVE)                    (None, 880, 50)      364         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 880, 50)      0           ['embedding_1[0][0]',            \n",
      "                                                                  'cve_2[0][0]',                  \n",
      "                                                                  'cve_3[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 880)          0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " transformer_1 (Transformer)    (None, 880, 50)      39508       ['add_1[0][0]',                  \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 880, 1)       5200        ['transformer_1[0][0]',          \n",
      "                                                                  'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          300         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 50)           0           ['transformer_1[0][0]',          \n",
      "                                                                  'attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50)           5050        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 100)          0           ['lambda_3[0][0]',               \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 133)          13433       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,919\n",
      "Trainable params: 70,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160648: 100%|██████████| 3200/3200 [02:40<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2064\n",
      "Epoch 0 loss 5.1391727309674025 val loss 5.206392288208008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160931: 100%|██████████| 3200/3200 [02:33<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.1945\n",
      "Epoch 1 loss 5.148235884830355 val loss 5.194498062133789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159453: 100%|██████████| 3200/3200 [02:37<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2769\n",
      "Epoch 2 loss 5.100957987718284 val loss 5.276888847351074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.158193: 100%|██████████| 3200/3200 [02:34<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.2235\n",
      "Epoch 3 loss 5.060655780993402 val loss 5.223454475402832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159393: 100%|██████████| 3200/3200 [02:33<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 70s 16ms/step - loss: 5.3821\n",
      "Epoch 4 loss 5.099045725539327 val loss 5.382086277008057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.159065: 100%|██████████| 3200/3200 [02:32<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.2131\n",
      "Epoch 5 loss 5.088524010255933 val loss 5.213129043579102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.161093: 100%|██████████| 3200/3200 [02:33<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.2089\n",
      "Epoch 6 loss 5.153402831442654 val loss 5.20894718170166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.156010: 100%|██████████| 3200/3200 [02:32<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260/4260 [==============================] - 69s 16ms/step - loss: 5.2048\n",
      "Epoch 7 loss 4.990814888514579 val loss 5.204789161682129\n"
     ]
    }
   ],
   "source": [
    "# continue training after disconnected (4h runtime limit with uni-cluster, and 24h runtime limit with colab)\n",
    "\n",
    "fore_loadpath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_21epochs.h5'\n",
    "\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 102400, 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print(fore_model.summary())\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "fore_model.load_weights(fore_loadpath)\n",
    "\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "\n",
    "# save losses for visualization\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for e in range(1000):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        e_loss += fore_model.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    val_loss = fore_model.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    val_losses.append(val_loss)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    train_losses.append(e_loss*batch_size/samples_per_epoch)\n",
    "#     # model should be saved here after each epoch in case of unexpected disconnection\n",
    "#     files.download(model_path)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        fore_model.save_weights(fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJP9Q8GvwyND"
   },
   "source": [
    "### visualize loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(len(val_losses))\n",
    "y = val_losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val loss')\n",
    "# plt.xticks(x, [str(i) for i in y], rotation=90)\n",
    "\n",
    "#set parameters for tick labels\n",
    "plt.tick_params(axis='x', which='major', labelsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('sepsis_pretext_large_val_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(len(train_losses))\n",
    "y = train_losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('train loss')\n",
    "# plt.xticks(x, [str(i) for i in y], rotation=90)\n",
    "\n",
    "#set parameters for tick labels\n",
    "plt.tick_params(axis='x', which='major', labelsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('sepsis_pretext_large_train_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFKXwwbkc0__"
   },
   "source": [
    "## Target Task\n",
    "\n",
    "Fine-tune: binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNM0wgujgFXa"
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LgViGaFFc9EZ",
    "outputId": "54050d37-662a-4142-999a-9f3a9e6aff75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_10ld.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 15:27:12.137465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 15:27:14.776253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30940 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n",
      "2023-03-10 15:27:14.776923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30926 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2023-03-10 15:27:14.777468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30933 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n",
      "2023-03-10 15:27:14.777955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30945 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 15:27:26.897345: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1466f807abd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-10 15:27:26.897375: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-10 15:27:26.897380: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-10 15:27:26.897384: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-10 15:27:26.897388: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-03-10 15:27:26.904470: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-10 15:27:27.075625: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - ETA: 0s - loss: 0.5272val_aucs: 0.40362259486585483 0.8594732704402517\n",
      "115/115 [==============================] - 17s 67ms/step - loss: 0.5272 - custom_metric: 1.2631\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4171val_aucs: 0.40789104221844125 0.875831519109821\n",
      "115/115 [==============================] - 6s 50ms/step - loss: 0.4171 - custom_metric: 1.2837\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3999val_aucs: 0.4603330657010475 0.8834512578616351\n",
      "115/115 [==============================] - 5s 48ms/step - loss: 0.3999 - custom_metric: 1.3438\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3727val_aucs: 0.4510989628885695 0.879414610546686\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3727 - custom_metric: 1.3305\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3458val_aucs: 0.436270417517693 0.8781144170295114\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3458 - custom_metric: 1.3144\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2951val_aucs: 0.4520733222977352 0.8715832123850992\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2951 - custom_metric: 1.3237\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2655val_aucs: 0.4016091522562731 0.858399854862119\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.2655 - custom_metric: 1.2600\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2465val_aucs: 0.4076254375418914 0.8558901790033866\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2465 - custom_metric: 1.2635\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2157val_aucs: 0.42058025172627655 0.8491473149492019\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2157 - custom_metric: 1.2697\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1859val_aucs: 0.418286714030794 0.8638727624576681\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1859 - custom_metric: 1.2822\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1713val_aucs: 0.39832939913278914 0.835948838896952\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1713 - custom_metric: 1.2343\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1692val_aucs: 0.3843385713790087 0.8505533381712628\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1692 - custom_metric: 1.2349\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1187val_aucs: 0.33771575810499266 0.8284046927914853\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1187 - custom_metric: 1.1661\n",
      "Test res 0.8579732190940641 0.4159527362765191 0.4496124031007752\n",
      "Repeat 1 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5535val_aucs: 0.4232858793543685 0.840555396916048\n",
      "115/115 [==============================] - 12s 60ms/step - loss: 0.5535 - custom_metric: 1.2638\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4613val_aucs: 0.4540655146322768 0.8544403198172472\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4613 - custom_metric: 1.3085\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4147val_aucs: 0.4754966080890356 0.8599133828288598\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4147 - custom_metric: 1.3354\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3824val_aucs: 0.4454698777712082 0.851275461640967\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3824 - custom_metric: 1.2967\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3480val_aucs: 0.5049260869245363 0.8731796116504854\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3480 - custom_metric: 1.3781\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3118val_aucs: 0.46636458901639966 0.860329811536265\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3118 - custom_metric: 1.3267\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2567val_aucs: 0.4430291945462562 0.840555396916048\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2567 - custom_metric: 1.2836\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2213val_aucs: 0.4679291787438896 0.8539644012944984\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2213 - custom_metric: 1.3219\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1921val_aucs: 0.46778418565767815 0.839948600799543\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1921 - custom_metric: 1.3077\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1816val_aucs: 0.46800745350824696 0.8409599276603846\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1816 - custom_metric: 1.3090\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1222val_aucs: 0.4515343933888464 0.8309894346087949\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1222 - custom_metric: 1.2825\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1073val_aucs: 0.4679234760442393 0.850418808300019\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1073 - custom_metric: 1.3183\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0976val_aucs: 0.44916827610929994 0.8315843327622311\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.0976 - custom_metric: 1.2808\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1018val_aucs: 0.42324877085175167 0.8288240053302874\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1018 - custom_metric: 1.2521\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0946val_aucs: 0.42722833646295433 0.8258138206739006\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0946 - custom_metric: 1.2530\n",
      "Test res 0.8604851678434658 0.42700648537534225 0.4571150097465887\n",
      "Repeat 2 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5435val_aucs: 0.47613247070395515 0.8663879916397491\n",
      "115/115 [==============================] - 12s 59ms/step - loss: 0.5435 - custom_metric: 1.3425\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4334val_aucs: 0.49740530397809873 0.8863259231110265\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4334 - custom_metric: 1.3837\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3817val_aucs: 0.5376504481386648 0.8878079675723605\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3817 - custom_metric: 1.4255\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3508val_aucs: 0.5278076464827979 0.8895560200139337\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3508 - custom_metric: 1.4174\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3270val_aucs: 0.5635634668846154 0.8911394008486921\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3270 - custom_metric: 1.4547\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2930val_aucs: 0.5306889812673701 0.881461777186649\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2930 - custom_metric: 1.4122\n",
      "Epoch 7/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2524val_aucs: 0.5443744725835347 0.8758376084615872\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2522 - custom_metric: 1.4202\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2262val_aucs: 0.5300177006572444 0.8834125023750712\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2262 - custom_metric: 1.4134\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2251val_aucs: 0.5307202038453299 0.8779403382101463\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2251 - custom_metric: 1.4087\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1785val_aucs: 0.46028847882773366 0.8683387168281715\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1785 - custom_metric: 1.3286\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1333val_aucs: 0.5130049326177544 0.8665146621065299\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1333 - custom_metric: 1.3795\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1176val_aucs: 0.5019536831458867 0.8714421432642979\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1176 - custom_metric: 1.3734\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1119val_aucs: 0.50391109618335 0.8736588764329597\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1119 - custom_metric: 1.3776\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0951val_aucs: 0.45948031714420684 0.8620051934891381\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.0951 - custom_metric: 1.3215\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1053val_aucs: 0.4702941750173467 0.8627778833365\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1053 - custom_metric: 1.3331\n",
      "Test res 0.8524824041108185 0.40807817135025914 0.42681775259678945\n",
      "Repeat 3 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5780val_aucs: 0.4193669208457765 0.8586422593361769\n",
      "115/115 [==============================] - 13s 60ms/step - loss: 0.5780 - custom_metric: 1.2780\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4828val_aucs: 0.479447235116831 0.8885627945730169\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4828 - custom_metric: 1.3680\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4421val_aucs: 0.48840289639733403 0.9024162759752776\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4421 - custom_metric: 1.3908\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4042val_aucs: 0.48824034222392826 0.8976906719062838\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4042 - custom_metric: 1.3859\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3621val_aucs: 0.5189205709673641 0.9006006491487695\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3621 - custom_metric: 1.4195\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3744val_aucs: 0.5215911666229466 0.901918843968015\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3744 - custom_metric: 1.4235\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2956val_aucs: 0.49711272039286836 0.8879161329635756\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2956 - custom_metric: 1.3850\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2719val_aucs: 0.478569296619339 0.8877917749617599\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2719 - custom_metric: 1.3664\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2367val_aucs: 0.5283268331330838 0.8923557136283935\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2367 - custom_metric: 1.4207\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2038val_aucs: 0.444470951101415 0.8630569684006317\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2038 - custom_metric: 1.3075\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1684val_aucs: 0.4824594270201508 0.8774576250108813\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1684 - custom_metric: 1.3599\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1227val_aucs: 0.49867252930332145 0.8696603782970416\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1227 - custom_metric: 1.3683\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1384val_aucs: 0.5037510092750731 0.8858393543332547\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1384 - custom_metric: 1.3896\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1162val_aucs: 0.4825170682226664 0.8715008767239129\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1162 - custom_metric: 1.3540\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1304val_aucs: 0.4541866579918733 0.8556825388929651\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1304 - custom_metric: 1.3099\n",
      "Epoch 16/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2320val_aucs: 0.4181078361364778 0.8350018031910263\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2320 - custom_metric: 1.2531\n",
      "Test res 0.8699178546852562 0.45459170802355825 0.46576663452266154\n",
      "Repeat 4 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5164val_aucs: 0.5030910825015418 0.8686821816105083\n",
      "115/115 [==============================] - 12s 59ms/step - loss: 0.5164 - custom_metric: 1.3718\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4401val_aucs: 0.5343589604246626 0.8802113078241005\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4401 - custom_metric: 1.4146\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3964val_aucs: 0.5309635761513937 0.8757376737102608\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3964 - custom_metric: 1.4067\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3667val_aucs: 0.5287704803078299 0.875594898153436\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3667 - custom_metric: 1.4044\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3217val_aucs: 0.5357840425803999 0.8868146773272416\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3217 - custom_metric: 1.4226\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2847val_aucs: 0.5638989866827985 0.8750594898153436\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2847 - custom_metric: 1.4390\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2645val_aucs: 0.5606518366329638 0.8821090329335617\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2645 - custom_metric: 1.4428\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2473val_aucs: 0.5279239370573378 0.8686702836474396\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2473 - custom_metric: 1.3966\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1987val_aucs: 0.534470558057688 0.8697767942128307\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1987 - custom_metric: 1.4042\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1679val_aucs: 0.5193834928175403 0.8781172663240052\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1679 - custom_metric: 1.3975\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1652val_aucs: 0.49309416444647275 0.8609366076527698\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1652 - custom_metric: 1.3540\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1167val_aucs: 0.5218578539882726 0.8667904054825815\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1167 - custom_metric: 1.3886\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2258val_aucs: 0.5005186061073821 0.863934894346088\n",
      "115/115 [==============================] - 6s 56ms/step - loss: 0.2258 - custom_metric: 1.3645\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1466val_aucs: 0.4881256765476257 0.8564391776127926\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1466 - custom_metric: 1.3446\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0950val_aucs: 0.5093998713636992 0.8621264039596421\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0950 - custom_metric: 1.3715\n",
      "Epoch 16/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0713val_aucs: 0.48729422125347277 0.8545711974110032\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.0713 - custom_metric: 1.3419\n",
      "Epoch 17/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0498val_aucs: 0.5018384428279357 0.8619122406244052\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0498 - custom_metric: 1.3638\n",
      "Test res 0.8527716797809956 0.40938432319559137 0.4526829268292683\n",
      "Repeat 5 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5860val_aucs: 0.3778621274941457 0.8286281932530573\n",
      "115/115 [==============================] - 12s 59ms/step - loss: 0.5860 - custom_metric: 1.2065\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4522val_aucs: 0.374097899114244 0.8399715409501565\n",
      "115/115 [==============================] - 6s 49ms/step - loss: 0.4522 - custom_metric: 1.2141\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4069val_aucs: 0.41733961289504057 0.8440658853852041\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.4069 - custom_metric: 1.2614\n",
      "Epoch 4/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.3773val_aucs: 0.36371382423190496 0.8309505591129369\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3771 - custom_metric: 1.1947\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3468val_aucs: 0.33340430765615936 0.8288832507752406\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3468 - custom_metric: 1.1623\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3346val_aucs: 0.38497794003160846 0.8360651336367175\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3346 - custom_metric: 1.2210\n",
      "Epoch 7/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2728val_aucs: 0.37809947402767397 0.8352328406695931\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2728 - custom_metric: 1.2133\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2648val_aucs: 0.33469161701456324 0.818962855570322\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2648 - custom_metric: 1.1537\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2281val_aucs: 0.33665321818630695 0.8112440095042487\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2281 - custom_metric: 1.1479\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1863val_aucs: 0.3486834563135205 0.8061160109003531\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1863 - custom_metric: 1.1548\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1443val_aucs: 0.34359651255283336 0.8162646154672251\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1443 - custom_metric: 1.1599\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1611val_aucs: 0.34033558613228077 0.8132576215214853\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1611 - custom_metric: 1.1536\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1270val_aucs: 0.32329886423790044 0.8182379552441169\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1270 - custom_metric: 1.1415\n",
      "Test res 0.8600249353009813 0.4296550310911977 0.443359375\n",
      "Repeat 6 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5188val_aucs: 0.36660384724927053 0.8417389442124424\n",
      "115/115 [==============================] - 13s 59ms/step - loss: 0.5188 - custom_metric: 1.2083\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4354val_aucs: 0.3932926451182102 0.85946797607575\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4354 - custom_metric: 1.2528\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4119val_aucs: 0.40829438007907637 0.8591926330442232\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4119 - custom_metric: 1.2675\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3789val_aucs: 0.43232024517882667 0.8633686690223792\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3789 - custom_metric: 1.2957\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3472val_aucs: 0.3979276048383119 0.8701604638000398\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3472 - custom_metric: 1.2681\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3291val_aucs: 0.4462044802003616 0.8669175347620578\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3291 - custom_metric: 1.3131\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3020val_aucs: 0.4620718833293933 0.8591620393740536\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3020 - custom_metric: 1.3212\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2604val_aucs: 0.36751254266583055 0.8338916678139293\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2604 - custom_metric: 1.2014\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2957val_aucs: 0.4435586844184164 0.851850152203509\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2957 - custom_metric: 1.2954\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2054val_aucs: 0.4098731207814824 0.8406834625915898\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2054 - custom_metric: 1.2506\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1738val_aucs: 0.44340090240490826 0.8367062854695363\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1738 - custom_metric: 1.2801\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1888val_aucs: 0.4323228632399745 0.8378535481008978\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1888 - custom_metric: 1.2702\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1241val_aucs: 0.42335910703709106 0.8484236611445093\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1241 - custom_metric: 1.2718\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0914val_aucs: 0.3875975963368214 0.8352377893013935\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0914 - custom_metric: 1.2228\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1085val_aucs: 0.38584859560849577 0.8216083092408182\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1085 - custom_metric: 1.2075\n",
      "Epoch 16/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1329val_aucs: 0.4297426973532971 0.8444005935172013\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1329 - custom_metric: 1.2741\n",
      "Epoch 17/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1060val_aucs: 0.40702218805773327 0.8314288773652732\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1060 - custom_metric: 1.2385\n",
      "Test res 0.8588765744674485 0.4266187104533249 0.44401168451801365\n",
      "Repeat 7 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5617val_aucs: 0.3856559416447568 0.8300980527128937\n",
      "115/115 [==============================] - 13s 59ms/step - loss: 0.5617 - custom_metric: 1.2158\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4729val_aucs: 0.3871121227737611 0.8367674728098756\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4729 - custom_metric: 1.2239\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4345val_aucs: 0.4116609593299047 0.8437428296085541\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4345 - custom_metric: 1.2554\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3954val_aucs: 0.3740875111967103 0.8435133770822817\n",
      "115/115 [==============================] - 8s 69ms/step - loss: 0.3954 - custom_metric: 1.2176\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3553val_aucs: 0.33087084862111493 0.8447218270539825\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3553 - custom_metric: 1.1756\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3252val_aucs: 0.39013855546955056 0.8522325730806296\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3252 - custom_metric: 1.2424\n",
      "Epoch 7/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2837val_aucs: 0.3541257459233479 0.839459715784804\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2841 - custom_metric: 1.1936\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2676val_aucs: 0.3858896747082601 0.8468021966255181\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2676 - custom_metric: 1.2327\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2259val_aucs: 0.3375112942343376 0.8489284567023082\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2259 - custom_metric: 1.1864\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1801val_aucs: 0.30542338743759684 0.82612087559084\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1801 - custom_metric: 1.1315\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2000val_aucs: 0.3731605978266884 0.8400256986829425\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.2000 - custom_metric: 1.2132\n",
      "Epoch 12/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1504val_aucs: 0.3209724466992945 0.8306181451057777\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1503 - custom_metric: 1.1516\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1149val_aucs: 0.30451129507174457 0.830067459042724\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1149 - custom_metric: 1.1346\n",
      "Test res 0.8686243885381761 0.42474850782144374 0.44921875\n",
      "Repeat 8 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5307val_aucs: 0.4929890017701724 0.883166879015796\n",
      "115/115 [==============================] - 12s 60ms/step - loss: 0.5307 - custom_metric: 1.3762\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4438val_aucs: 0.5282399789080858 0.8935925362706013\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4438 - custom_metric: 1.4218\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4074val_aucs: 0.5355973585337994 0.8898387516611183\n",
      "115/115 [==============================] - 6s 48ms/step - loss: 0.4074 - custom_metric: 1.4254\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3838val_aucs: 0.5445253919848687 0.8934281369446385\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.3838 - custom_metric: 1.4380\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3568val_aucs: 0.5703460311992297 0.8955927280698148\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3568 - custom_metric: 1.4659\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3487val_aucs: 0.5994630665627284 0.8940857342484896\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3487 - custom_metric: 1.4935\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2901val_aucs: 0.5512339205571682 0.8731933199073884\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2901 - custom_metric: 1.4244\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2791val_aucs: 0.5251113552740521 0.8769060046853808\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2791 - custom_metric: 1.4020\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2175val_aucs: 0.4692606031334045 0.8561094899510913\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.2175 - custom_metric: 1.3254\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1835val_aucs: 0.4867851721241946 0.8635348595070761\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.1835 - custom_metric: 1.3503\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1452val_aucs: 0.47554799144881205 0.8527804036003452\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1452 - custom_metric: 1.3283\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1425val_aucs: 0.469643876907845 0.856451988546847\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1425 - custom_metric: 1.3261\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1169val_aucs: 0.47397765671850034 0.8602331730439905\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1169 - custom_metric: 1.3342\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1816val_aucs: 0.3972605134695556 0.8418478484238215\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1816 - custom_metric: 1.2391\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1271val_aucs: 0.4354458117079269 0.8555203923663912\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1271 - custom_metric: 1.2910\n",
      "Epoch 16/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0652val_aucs: 0.45194146253365464 0.8589316783801186\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0652 - custom_metric: 1.3109\n",
      "Test res 0.8642770726124939 0.43448024186420475 0.4580078125\n",
      "Repeat 9 ld 10\n",
      "Num train: 3655 Num valid: 926\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_10ld.h5\n",
      "Epoch 1/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5554val_aucs: 0.49337516502369794 0.8750968523002421\n",
      "115/115 [==============================] - 12s 60ms/step - loss: 0.5554 - custom_metric: 1.3685\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4469val_aucs: 0.5304888773493309 0.8975544794188861\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.4469 - custom_metric: 1.4280\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4091val_aucs: 0.5572417648023976 0.903910411622276\n",
      "115/115 [==============================] - 5s 47ms/step - loss: 0.4091 - custom_metric: 1.4612\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3753val_aucs: 0.5954508384537482 0.9059806295399515\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3753 - custom_metric: 1.5014\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3471val_aucs: 0.5938640008046192 0.9035351089588377\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.3471 - custom_metric: 1.4974\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3010val_aucs: 0.5743309595185925 0.9080992736077482\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.3010 - custom_metric: 1.4824\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2814val_aucs: 0.5504693243779328 0.9034019370460049\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2814 - custom_metric: 1.4539\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2268val_aucs: 0.5190125834332784 0.8933777239709443\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2268 - custom_metric: 1.4124\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2078val_aucs: 0.5437582830742076 0.903135593220339\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.2078 - custom_metric: 1.4469\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1729val_aucs: 0.4806214934611467 0.893226392251816\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.1729 - custom_metric: 1.3738\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1472val_aucs: 0.5147850954324271 0.8874818401937046\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1472 - custom_metric: 1.4023\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1555val_aucs: 0.43232033499058986 0.8754479418886197\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1555 - custom_metric: 1.3078\n",
      "Epoch 13/1000\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1143val_aucs: 0.45211166272115105 0.8710048426150121\n",
      "115/115 [==============================] - 5s 46ms/step - loss: 0.1143 - custom_metric: 1.3231\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0821val_aucs: 0.49244800218982465 0.8846489104116222\n",
      "115/115 [==============================] - 5s 45ms/step - loss: 0.0821 - custom_metric: 1.3771\n",
      "Test res 0.8663553415958593 0.4277043177753421 0.443359375\n",
      "gen_res {10: [(0.8611788638029558, 0.005761069640297871), (0.4258220233226783, 0.012674895180699959), (0.4489951723814096, 0.010112711805031717)]}\n",
      "Repeat 0 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4920val_aucs: 0.4673601954393268 0.8643188643188643\n",
      "229/229 [==============================] - 20s 52ms/step - loss: 0.4920 - custom_metric: 1.3317\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4270val_aucs: 0.46657502293307046 0.8723771900242488\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4270 - custom_metric: 1.3390\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3877val_aucs: 0.45342355369017917 0.8671805495334908\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3877 - custom_metric: 1.3206\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3694val_aucs: 0.48154575471406713 0.8706492588845531\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3694 - custom_metric: 1.3522\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3440val_aucs: 0.4802764957829929 0.8722101780925311\n",
      "229/229 [==============================] - 10s 46ms/step - loss: 0.3440 - custom_metric: 1.3525\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3142val_aucs: 0.47003466417742285 0.8668593727417258\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3142 - custom_metric: 1.3369\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3019val_aucs: 0.45832601872282497 0.8651314416020299\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3019 - custom_metric: 1.3235\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2560val_aucs: 0.43989673361401077 0.8446275152157505\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2560 - custom_metric: 1.2845\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2468val_aucs: 0.4674683070964502 0.8605482487835429\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2468 - custom_metric: 1.3280\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2150val_aucs: 0.45480367842745156 0.8579370814664933\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2150 - custom_metric: 1.3127\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1758val_aucs: 0.3936596043097892 0.8463586581233641\n",
      "229/229 [==============================] - 12s 54ms/step - loss: 0.1758 - custom_metric: 1.2400\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1834val_aucs: 0.41057259461444606 0.8516002633649692\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1834 - custom_metric: 1.2622\n",
      "Epoch 13/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.1853val_aucs: 0.4235376661188395 0.8422475951887718\n",
      "229/229 [==============================] - 11s 48ms/step - loss: 0.1851 - custom_metric: 1.2658\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1443val_aucs: 0.44710968222007597 0.8545743604567134\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1443 - custom_metric: 1.3017\n",
      "Epoch 15/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1223val_aucs: 0.4414250282000366 0.8567872685519744\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1223 - custom_metric: 1.2982\n",
      "Test res 0.8728647453925323 0.45354074168734654 0.4659533073929961\n",
      "Repeat 1 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5029val_aucs: 0.47630174266429515 0.8815552946641341\n",
      "229/229 [==============================] - 18s 52ms/step - loss: 0.5029 - custom_metric: 1.3579\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4334val_aucs: 0.5108264482616202 0.8915805247735494\n",
      "229/229 [==============================] - 10s 46ms/step - loss: 0.4334 - custom_metric: 1.4024\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4028val_aucs: 0.5297251582248585 0.8845682724215497\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4028 - custom_metric: 1.4143\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3718val_aucs: 0.5397437644150254 0.8854623027804025\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3718 - custom_metric: 1.4252\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3597val_aucs: 0.5215346489218325 0.8874062620304353\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3597 - custom_metric: 1.4089\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3290val_aucs: 0.5198903081647516 0.8816348347316477\n",
      "229/229 [==============================] - 13s 55ms/step - loss: 0.3290 - custom_metric: 1.4015\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3217val_aucs: 0.5363569548446705 0.8905242326769687\n",
      "229/229 [==============================] - 11s 49ms/step - loss: 0.3217 - custom_metric: 1.4269\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2837val_aucs: 0.5120266000328396 0.8745844031472414\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2837 - custom_metric: 1.3866\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2532val_aucs: 0.5001867472870757 0.8730031466050707\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2532 - custom_metric: 1.3732\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2258val_aucs: 0.4123967995714402 0.8634106144629295\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2258 - custom_metric: 1.2758\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2320val_aucs: 0.44507224797909894 0.8680112119679166\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2320 - custom_metric: 1.3131\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1875val_aucs: 0.4511678842548753 0.8601621981056737\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1875 - custom_metric: 1.3113\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1749val_aucs: 0.3910075023354375 0.8486829755621097\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1749 - custom_metric: 1.2397\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1664val_aucs: 0.38604327191091314 0.8469076412552059\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1664 - custom_metric: 1.2330\n",
      "Epoch 15/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1591val_aucs: 0.42824761686549406 0.8558129472140295\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1591 - custom_metric: 1.2841\n",
      "Epoch 16/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1279val_aucs: 0.42360535615884887 0.8528254222782184\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1279 - custom_metric: 1.2764\n",
      "Epoch 17/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1257val_aucs: 0.45493011223580315 0.8529049623457319\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1257 - custom_metric: 1.3078\n",
      "Test res 0.8773904978458592 0.4667237377038688 0.4716796875\n",
      "Repeat 2 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5208val_aucs: 0.49982789523417365 0.8953771765196539\n",
      "229/229 [==============================] - 21s 53ms/step - loss: 0.5208 - custom_metric: 1.3952\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4438val_aucs: 0.5451742738569613 0.8963839797727037\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4438 - custom_metric: 1.4416\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4037val_aucs: 0.5326833580685028 0.9034283442810969\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4037 - custom_metric: 1.4361\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3844val_aucs: 0.5624706385484765 0.9064715618809299\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3844 - custom_metric: 1.4689\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3541val_aucs: 0.5641100232955322 0.9037639453654468\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3541 - custom_metric: 1.4679\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3388val_aucs: 0.5392172600979206 0.8956117714524032\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3388 - custom_metric: 1.4348\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3118val_aucs: 0.5608416593591499 0.892591361693254\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3118 - custom_metric: 1.4534\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2917val_aucs: 0.46176547743943486 0.8845923261390889\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2917 - custom_metric: 1.3464\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2585val_aucs: 0.5288933686535395 0.8853971170889375\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2585 - custom_metric: 1.4143\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2470val_aucs: 0.5194502938743686 0.8831228495464499\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2470 - custom_metric: 1.4026\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2125val_aucs: 0.4656775347722295 0.8779226618705036\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2125 - custom_metric: 1.3436\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2040val_aucs: 0.5277402823174494 0.8907341518089875\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2040 - custom_metric: 1.4185\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1790val_aucs: 0.458764708358667 0.8707512251068711\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1790 - custom_metric: 1.3295\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1983val_aucs: 0.512899878338995 0.866482900636013\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1983 - custom_metric: 1.3794\n",
      "Test res 0.8795570735100526 0.4776571218592591 0.4765625\n",
      "Repeat 3 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_20ld.h5\n",
      "Epoch 1/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.4716val_aucs: 0.5269811376468557 0.9090151927708388\n",
      "229/229 [==============================] - 18s 52ms/step - loss: 0.4716 - custom_metric: 1.4360\n",
      "Epoch 2/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.4038val_aucs: 0.5627351941396052 0.9089103547892028\n",
      "229/229 [==============================] - 10s 46ms/step - loss: 0.4048 - custom_metric: 1.4716\n",
      "Epoch 3/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.3718val_aucs: 0.5809797666991726 0.9131708538762389\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3715 - custom_metric: 1.4942\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3561val_aucs: 0.5730956903694355 0.9081415429238731\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3561 - custom_metric: 1.4812\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3294val_aucs: 0.5623237160447407 0.9082696782347613\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3294 - custom_metric: 1.4706\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3165val_aucs: 0.5637577045667439 0.9048245856715601\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3165 - custom_metric: 1.4686\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2822val_aucs: 0.5740831103242989 0.907049480615166\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2822 - custom_metric: 1.4811\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2662val_aucs: 0.5428624233512267 0.8936709892919651\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2662 - custom_metric: 1.4365\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2372val_aucs: 0.5266986013679125 0.8913266955359405\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2372 - custom_metric: 1.4180\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2255val_aucs: 0.508963750315058 0.8897745109744981\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2255 - custom_metric: 1.3987\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1928val_aucs: 0.5319926791712131 0.8857033026876381\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1928 - custom_metric: 1.4177\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1729val_aucs: 0.5099269444109154 0.8824300279276734\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1729 - custom_metric: 1.3924\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1516val_aucs: 0.4804353585796908 0.8809564718524581\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1516 - custom_metric: 1.3614\n",
      "Test res 0.8727279611955482 0.4639886012812831 0.4819512195121951\n",
      "Repeat 4 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5064val_aucs: 0.4372622184876209 0.8724032695876209\n",
      "229/229 [==============================] - 18s 52ms/step - loss: 0.5064 - custom_metric: 1.3097\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4243val_aucs: 0.4595551587762566 0.8788136243041391\n",
      "229/229 [==============================] - 10s 46ms/step - loss: 0.4243 - custom_metric: 1.3384\n",
      "Epoch 3/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.3961val_aucs: 0.48839498331955716 0.8877666508196973\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3955 - custom_metric: 1.3762\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3732val_aucs: 0.48634508551413824 0.8794025181345563\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3732 - custom_metric: 1.3657\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3510val_aucs: 0.4956274771126255 0.883825356173417\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3510 - custom_metric: 1.3795\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3265val_aucs: 0.4663830144558522 0.8783167451347249\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3265 - custom_metric: 1.3447\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2943val_aucs: 0.4736472495931931 0.8801785084423451\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2943 - custom_metric: 1.3538\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2781val_aucs: 0.4457492965176389 0.8652537304277148\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2781 - custom_metric: 1.3110\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2477val_aucs: 0.4201285432935294 0.8617541061542472\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2477 - custom_metric: 1.2819\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2262val_aucs: 0.40522067236168285 0.8363979327372828\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2262 - custom_metric: 1.2416\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2073val_aucs: 0.47184998744056095 0.8678945511984909\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2073 - custom_metric: 1.3397\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1810val_aucs: 0.41700909259842606 0.8518717315625622\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1810 - custom_metric: 1.2689\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1836val_aucs: 0.42414285535998664 0.8617050316683792\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1836 - custom_metric: 1.2858\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1366val_aucs: 0.4136325660868283 0.8545677611299399\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1366 - custom_metric: 1.2682\n",
      "Epoch 15/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1173val_aucs: 0.4006759906103641 0.846740380633981\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1173 - custom_metric: 1.2474\n",
      "Test res 0.8729157472923647 0.44669762820022474 0.4599609375\n",
      "Repeat 5 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5039val_aucs: 0.45511271035379786 0.8813760379596679\n",
      "229/229 [==============================] - 18s 53ms/step - loss: 0.5039 - custom_metric: 1.3365\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4383val_aucs: 0.46629139968959227 0.8864675784990497\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4383 - custom_metric: 1.3528\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4090val_aucs: 0.4753167692374838 0.8899048149894954\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4090 - custom_metric: 1.3652\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3839val_aucs: 0.49389054037284297 0.895978933527705\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3839 - custom_metric: 1.3899\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3551val_aucs: 0.4771965731731893 0.8862817819319985\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3551 - custom_metric: 1.3635\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3348val_aucs: 0.48866451754781276 0.8881647586788435\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3348 - custom_metric: 1.3768\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3052val_aucs: 0.4717093896366913 0.8910553245008502\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3052 - custom_metric: 1.3628\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2883val_aucs: 0.4587703160414973 0.8849240377881633\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2883 - custom_metric: 1.3437\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2527val_aucs: 0.45284574591705823 0.8864997355971931\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2527 - custom_metric: 1.3393\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2240val_aucs: 0.4327154925004858 0.8872357758435879\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2240 - custom_metric: 1.3200\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2092val_aucs: 0.38791059600531097 0.8723756234904029\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2092 - custom_metric: 1.2603\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1849val_aucs: 0.402670703252306 0.8664587174320056\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1849 - custom_metric: 1.2691\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1581val_aucs: 0.4140347749497442 0.8724256456430705\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1581 - custom_metric: 1.2865\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1451val_aucs: 0.39916733161785345 0.8570831368177335\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1451 - custom_metric: 1.2563\n",
      "Test res 0.8798899181725706 0.4530201546776278 0.4775390625\n",
      "Repeat 6 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4746val_aucs: 0.4735859008605135 0.9038458084101237\n",
      "229/229 [==============================] - 17s 53ms/step - loss: 0.4746 - custom_metric: 1.3774\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4170val_aucs: 0.49135721387325415 0.9109266943291839\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4170 - custom_metric: 1.4023\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3833val_aucs: 0.4905715084711398 0.9107003646422733\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3833 - custom_metric: 1.4013\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3601val_aucs: 0.47150929038623274 0.8992617341165059\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3601 - custom_metric: 1.3708\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3363val_aucs: 0.4724277157212084 0.900016166406208\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3363 - custom_metric: 1.3724\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3049val_aucs: 0.477218026778555 0.9036122936537875\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3049 - custom_metric: 1.3808\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2837val_aucs: 0.47568673369743525 0.895838048535144\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2837 - custom_metric: 1.3715\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2593val_aucs: 0.45020124292780844 0.8921664780585942\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2593 - custom_metric: 1.3424\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2247val_aucs: 0.41090606788749007 0.89112105045715\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2247 - custom_metric: 1.3020\n",
      "Epoch 10/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.2044val_aucs: 0.4251628075053372 0.8748001652565968\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2044 - custom_metric: 1.3000\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2078val_aucs: 0.4366770104148991 0.8779328555262166\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2078 - custom_metric: 1.3146\n",
      "Epoch 12/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.1620val_aucs: 0.43172435256619773 0.8755545975462988\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1624 - custom_metric: 1.3073\n",
      "Test res 0.8811550083772139 0.4725358222389787 0.48828125\n",
      "Repeat 7 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4860val_aucs: 0.4604695860244939 0.8923807436627951\n",
      "229/229 [==============================] - 18s 53ms/step - loss: 0.4860 - custom_metric: 1.3529\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4247val_aucs: 0.4730806074665222 0.8954963839579224\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4247 - custom_metric: 1.3686\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3911val_aucs: 0.4824469321038774 0.8891883994448097\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3911 - custom_metric: 1.3716\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3649val_aucs: 0.48410787337124483 0.8955475199064943\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3649 - custom_metric: 1.3797\n",
      "Epoch 5/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.3304val_aucs: 0.46684446779655075 0.8935203448023961\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3300 - custom_metric: 1.3604\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3162val_aucs: 0.47319590024252417 0.8907845715538023\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3162 - custom_metric: 1.3640\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2958val_aucs: 0.47454343240286806 0.8913726349623786\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2958 - custom_metric: 1.3659\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2584val_aucs: 0.4227459609873222 0.8825516838337352\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2584 - custom_metric: 1.3053\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2455val_aucs: 0.4877964640284983 0.8820951128643436\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2455 - custom_metric: 1.3699\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2114val_aucs: 0.4210108401887893 0.874545255314486\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2114 - custom_metric: 1.2956\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2061val_aucs: 0.5364793593919349 0.8866644751260135\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2061 - custom_metric: 1.4231\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1542val_aucs: 0.46713003308340056 0.8874570823288772\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1542 - custom_metric: 1.3546\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1500val_aucs: 0.46554176286481314 0.8746986631602016\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1500 - custom_metric: 1.3402\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1400val_aucs: 0.42964452193875363 0.8680144641683103\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1400 - custom_metric: 1.2977\n",
      "Epoch 15/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1190val_aucs: 0.44937667163442435 0.8804076265614728\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1190 - custom_metric: 1.3298\n",
      "Epoch 16/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1079val_aucs: 0.47395162995863016 0.878862590401052\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1079 - custom_metric: 1.3528\n",
      "Epoch 17/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.0847val_aucs: 0.46764204257024045 0.8824201913945504\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.0847 - custom_metric: 1.3501\n",
      "Epoch 18/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1008val_aucs: 0.41237087305671416 0.8639491562568485\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1008 - custom_metric: 1.2763\n",
      "Epoch 19/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.0897val_aucs: 0.41885525779796984 0.8662210534005406\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.0897 - custom_metric: 1.2851\n",
      "Epoch 20/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.0763val_aucs: 0.4187040542537253 0.8622890642121412\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.0763 - custom_metric: 1.2810\n",
      "Epoch 21/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1097val_aucs: 0.4328181551619406 0.8729636934765138\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1097 - custom_metric: 1.3058\n",
      "Test res 0.8648396829374103 0.43330593841982407 0.462890625\n",
      "Repeat 8 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5069val_aucs: 0.4451727105401677 0.8898381278985057\n",
      "229/229 [==============================] - 18s 53ms/step - loss: 0.5069 - custom_metric: 1.3350\n",
      "Epoch 2/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4241val_aucs: 0.44551765956504324 0.8846567991613212\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4241 - custom_metric: 1.3302\n",
      "Epoch 3/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3978val_aucs: 0.49726108958590026 0.8992483874980011\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3978 - custom_metric: 1.3965\n",
      "Epoch 4/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.3650val_aucs: 0.46822830873230104 0.8874642406581496\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3648 - custom_metric: 1.3557\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3390val_aucs: 0.509164710550793 0.8949803656781393\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3390 - custom_metric: 1.4041\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3141val_aucs: 0.5201462797421359 0.8974430960038381\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3141 - custom_metric: 1.4176\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2944val_aucs: 0.524974907376123 0.887858703957071\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2944 - custom_metric: 1.4128\n",
      "Epoch 8/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.2671val_aucs: 0.4355596122460619 0.8800369587235025\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2667 - custom_metric: 1.3156\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2473val_aucs: 0.46658256242092677 0.8816254730894295\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2473 - custom_metric: 1.3482\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2198val_aucs: 0.47843094965507194 0.8833312603280087\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2198 - custom_metric: 1.3618\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1812val_aucs: 0.48561056836945493 0.8863732475701417\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1812 - custom_metric: 1.3720\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1636val_aucs: 0.4594191773924029 0.8767746406297198\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1636 - custom_metric: 1.3362\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1478val_aucs: 0.4655988222383025 0.8801364629790863\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1478 - custom_metric: 1.3457\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1275val_aucs: 0.47145179203821724 0.8802963805327031\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1275 - custom_metric: 1.3517\n",
      "Epoch 15/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1331val_aucs: 0.4756265923227307 0.8776595177597327\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1331 - custom_metric: 1.3533\n",
      "Epoch 16/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1196val_aucs: 0.4472463357247014 0.8774391869080829\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1196 - custom_metric: 1.3247\n",
      "Test res 0.8752393022229535 0.47367656221845156 0.4712195121951219\n",
      "Repeat 9 ld 20\n",
      "Num train: 7310 Num valid: 1852\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_20ld.h5\n",
      "Epoch 1/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.5368val_aucs: 0.43490271705202466 0.8739545997610513\n",
      "229/229 [==============================] - 18s 52ms/step - loss: 0.5368 - custom_metric: 1.3089\n",
      "Epoch 2/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.4563val_aucs: 0.4954512069457011 0.8857073819016552\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4561 - custom_metric: 1.3812\n",
      "Epoch 3/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.4280val_aucs: 0.49371831602734134 0.8856100573208221\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4276 - custom_metric: 1.3793\n",
      "Epoch 4/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.4014val_aucs: 0.508514098940523 0.88516706267703\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.4014 - custom_metric: 1.3937\n",
      "Epoch 5/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3836val_aucs: 0.5258854689530418 0.8803578859758634\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3836 - custom_metric: 1.4062\n",
      "Epoch 6/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3475val_aucs: 0.5164387063311048 0.8826668277556281\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3475 - custom_metric: 1.3991\n",
      "Epoch 7/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3264val_aucs: 0.5024920157485534 0.8811129904823273\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3264 - custom_metric: 1.3836\n",
      "Epoch 8/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3121val_aucs: 0.5283186290841139 0.8776898500530252\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.3121 - custom_metric: 1.4060\n",
      "Epoch 9/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2813val_aucs: 0.5144354261206777 0.8656685863101231\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2813 - custom_metric: 1.3801\n",
      "Epoch 10/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2563val_aucs: 0.44806106727085804 0.8523485428161035\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.2563 - custom_metric: 1.3004\n",
      "Epoch 11/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2098val_aucs: 0.4837705321134887 0.8546675526559543\n",
      "229/229 [==============================] - 10s 46ms/step - loss: 0.2098 - custom_metric: 1.3384\n",
      "Epoch 12/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1983val_aucs: 0.48494297180832985 0.852207589974897\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1983 - custom_metric: 1.3372\n",
      "Epoch 13/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1709val_aucs: 0.4774955629063411 0.8570251567261353\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1709 - custom_metric: 1.3345\n",
      "Epoch 14/1000\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1563val_aucs: 0.4742248609153598 0.8317325117796303\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1563 - custom_metric: 1.3060\n",
      "Epoch 15/1000\n",
      "228/229 [============================>.] - ETA: 0s - loss: 0.1392val_aucs: 0.40701580496219675 0.8340313854993087\n",
      "229/229 [==============================] - 10s 45ms/step - loss: 0.1397 - custom_metric: 1.2410\n",
      "Test res 0.8697561071385831 0.4346854163146636 0.4638671875\n",
      "gen_res {10: [(0.8611788638029558, 0.005761069640297871), (0.4258220233226783, 0.012674895180699959), (0.4489951723814096, 0.010112711805031717)], 20: [(0.8746336044085087, 0.00481443321810223), (0.4575831724601528, 0.015097915769981042), (0.47199052891003135, 0.008642564577024284)]}\n",
      "Repeat 0 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_30ld.h5\n",
      "Epoch 1/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4834val_aucs: 0.4434810268735753 0.8745711123836042\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4831 - custom_metric: 1.3181\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4293val_aucs: 0.45609571460546455 0.8795580161661499\n",
      "343/343 [==============================] - 16s 45ms/step - loss: 0.4292 - custom_metric: 1.3357\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4085val_aucs: 0.49767793790284603 0.88788341356728\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4085 - custom_metric: 1.3856\n",
      "Epoch 4/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3951val_aucs: 0.47191812837087227 0.8847610036780538\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3948 - custom_metric: 1.3567\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3719val_aucs: 0.4999553222995111 0.8834243714664523\n",
      "343/343 [==============================] - 16s 45ms/step - loss: 0.3719 - custom_metric: 1.3834\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3621val_aucs: 0.4882009435173878 0.8787893754717184\n",
      "343/343 [==============================] - 16s 45ms/step - loss: 0.3621 - custom_metric: 1.3670\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3421val_aucs: 0.4834671575649545 0.8822760407904343\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3421 - custom_metric: 1.3657\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3156val_aucs: 0.467743958164005 0.8654207381728886\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3156 - custom_metric: 1.3332\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2962val_aucs: 0.4833992122356582 0.8697941185095779\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2962 - custom_metric: 1.3532\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2822val_aucs: 0.4694436886485454 0.8704531738841205\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2822 - custom_metric: 1.3399\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2581val_aucs: 0.44559786855398964 0.8708837978876273\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2581 - custom_metric: 1.3165\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2377val_aucs: 0.4645342025141773 0.8708899717084659\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2377 - custom_metric: 1.3354\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2074val_aucs: 0.43425636625166397 0.8620737555506507\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2074 - custom_metric: 1.2963\n",
      "Test res 0.8866193640797032 0.48639106645969366 0.48927875243664715\n",
      "Repeat 1 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_30ld.h5\n",
      "Epoch 1/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.5176val_aucs: 0.46841004650028045 0.8648919135308247\n",
      "343/343 [==============================] - 24s 50ms/step - loss: 0.5176 - custom_metric: 1.3333\n",
      "Epoch 2/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4397val_aucs: 0.47797096470976985 0.8741550383163674\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4397 - custom_metric: 1.3521\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4149val_aucs: 0.5157591081905749 0.8844761523504518\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4149 - custom_metric: 1.4002\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3924val_aucs: 0.49338772276906917 0.8751872926912958\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3924 - custom_metric: 1.3686\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3756val_aucs: 0.5006839720261512 0.8848207137138282\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3756 - custom_metric: 1.3855\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3541val_aucs: 0.49936735547397065 0.8807546036829463\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3541 - custom_metric: 1.3801\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3348val_aucs: 0.49771260556763397 0.8853153951732814\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3348 - custom_metric: 1.3830\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3046val_aucs: 0.4838415220061395 0.8787186892371041\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3046 - custom_metric: 1.3626\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2785val_aucs: 0.47290247876238356 0.8587212627244653\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2785 - custom_metric: 1.3316\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2630val_aucs: 0.45354990798250516 0.8631676769987418\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2630 - custom_metric: 1.3167\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2499val_aucs: 0.44799871294114907 0.8632763353539976\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2499 - custom_metric: 1.3113\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2227val_aucs: 0.4521250636983291 0.8568168820770902\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2227 - custom_metric: 1.3089\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1974val_aucs: 0.4561934919575897 0.8678271188379275\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1974 - custom_metric: 1.3240\n",
      "Test res 0.8799409668202489 0.464868418312291 0.478515625\n",
      "Repeat 2 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_30ld.h5\n",
      "Epoch 1/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.5124val_aucs: 0.46694371630889786 0.8695969767981928\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.5126 - custom_metric: 1.3365\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4390val_aucs: 0.49818702098252915 0.8778868551241722\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4396 - custom_metric: 1.3761\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4142val_aucs: 0.5193753396431278 0.8812548820909072\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4142 - custom_metric: 1.4006\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3869val_aucs: 0.5108457988530659 0.8784526498757924\n",
      "343/343 [==============================] - 16s 45ms/step - loss: 0.3869 - custom_metric: 1.3893\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3675val_aucs: 0.4877894236058354 0.8788284389272419\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3675 - custom_metric: 1.3666\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3518val_aucs: 0.4801123625888354 0.8733464226149006\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3518 - custom_metric: 1.3535\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3380val_aucs: 0.49638936295533886 0.8792534887157727\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3380 - custom_metric: 1.3756\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3111val_aucs: 0.45680648199251184 0.8684893139386776\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3111 - custom_metric: 1.3253\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2899val_aucs: 0.4526172896927596 0.863837692908565\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2899 - custom_metric: 1.3165\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2740val_aucs: 0.42522968966771735 0.8504401798720627\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2740 - custom_metric: 1.2757\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2383val_aucs: 0.44313931167155585 0.8635604253312785\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2383 - custom_metric: 1.3067\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2113val_aucs: 0.4136363098881348 0.8443487378695435\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2113 - custom_metric: 1.2580\n",
      "Epoch 13/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2039val_aucs: 0.4246415527261614 0.8441333980760164\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2042 - custom_metric: 1.2688\n",
      "Test res 0.8882681606031594 0.49726635180448636 0.4912109375\n",
      "Repeat 3 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_30ld.h5\n",
      "Epoch 1/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4917val_aucs: 0.48532554380447257 0.8792478845243348\n",
      "343/343 [==============================] - 24s 51ms/step - loss: 0.4911 - custom_metric: 1.3646\n",
      "Epoch 2/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4259val_aucs: 0.5119539114831345 0.8904231219578929\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4259 - custom_metric: 1.4024\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4015val_aucs: 0.5407551971766709 0.8909113344429885\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4015 - custom_metric: 1.4317\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3902val_aucs: 0.528158616582088 0.8943449167557476\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3902 - custom_metric: 1.4225\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3655val_aucs: 0.5343943176204052 0.8928118759028243\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3655 - custom_metric: 1.4272\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3435val_aucs: 0.5250857432708362 0.886681053734222\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3435 - custom_metric: 1.4118\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3230val_aucs: 0.5227733158573876 0.891501481402994\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3230 - custom_metric: 1.4143\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2910val_aucs: 0.5174748099347329 0.8830436024716428\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2910 - custom_metric: 1.4005\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2805val_aucs: 0.5063641055622747 0.8857824208633046\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2805 - custom_metric: 1.3921\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2482val_aucs: 0.49363776790415487 0.8738373098955575\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2482 - custom_metric: 1.3675\n",
      "Epoch 11/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2301val_aucs: 0.5067596563816126 0.8809915005425328\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2300 - custom_metric: 1.3878\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2218val_aucs: 0.48796854810934726 0.8779790685603229\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2218 - custom_metric: 1.3659\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1881val_aucs: 0.4909558293205217 0.8714928169726266\n",
      "343/343 [==============================] - 16s 46ms/step - loss: 0.1881 - custom_metric: 1.3624\n",
      "Test res 0.8773384674934179 0.4667954557421287 0.4819512195121951\n",
      "Repeat 4 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_30ld.h5\n",
      "Epoch 1/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4982val_aucs: 0.47310046804707867 0.8880683813364824\n",
      "343/343 [==============================] - 23s 52ms/step - loss: 0.4982 - custom_metric: 1.3612\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4253val_aucs: 0.49825245112168937 0.9002312863283258\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4256 - custom_metric: 1.3985\n",
      "Epoch 3/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3900val_aucs: 0.5124124597548858 0.9012002886263726\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3908 - custom_metric: 1.4136\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3705val_aucs: 0.4898484318511033 0.8986696220742872\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3705 - custom_metric: 1.3885\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3586val_aucs: 0.5135843562047008 0.8958589227479956\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3586 - custom_metric: 1.4094\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3397val_aucs: 0.48773816422114136 0.8899811977994462\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3397 - custom_metric: 1.3777\n",
      "Epoch 7/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3193val_aucs: 0.48546907298007486 0.8929459892342955\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3192 - custom_metric: 1.3784\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3012val_aucs: 0.450455146307883 0.882325486982921\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3012 - custom_metric: 1.3328\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2727val_aucs: 0.4218494898193236 0.876982639449654\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2727 - custom_metric: 1.2988\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2525val_aucs: 0.4373284810024198 0.8829966766480818\n",
      "343/343 [==============================] - 17s 51ms/step - loss: 0.2525 - custom_metric: 1.3203\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2326val_aucs: 0.4439211771046126 0.8812912918697151\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2326 - custom_metric: 1.3252\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2080val_aucs: 0.45197380992941816 0.8897974725930887\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2080 - custom_metric: 1.3418\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1835val_aucs: 0.405078009688887 0.8649708632565886\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1835 - custom_metric: 1.2700\n",
      "Test res 0.8819310226184778 0.46664911366282447 0.4775390625\n",
      "Repeat 5 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_30ld.h5\n",
      "Epoch 1/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4943val_aucs: 0.46849133085235006 0.8865722266560255\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4947 - custom_metric: 1.3551\n",
      "Epoch 2/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4191val_aucs: 0.48341748021276687 0.8985332261396181\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4191 - custom_metric: 1.3820\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4000val_aucs: 0.5027702591341815 0.904226914229379\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4000 - custom_metric: 1.4070\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3807val_aucs: 0.49457472119217727 0.9006399347448478\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3807 - custom_metric: 1.3952\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3656val_aucs: 0.4753223513533627 0.8974681352988123\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3656 - custom_metric: 1.3728\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3516val_aucs: 0.48162328116076625 0.8982192737430168\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3516 - custom_metric: 1.3798\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3278val_aucs: 0.48222020507297314 0.8992418196328811\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3278 - custom_metric: 1.3815\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3123val_aucs: 0.4834334856733098 0.9030767334866907\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3123 - custom_metric: 1.3865\n",
      "Epoch 9/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2872val_aucs: 0.4705080425269008 0.8935994788977043\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2874 - custom_metric: 1.3641\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2666val_aucs: 0.4635245049664939 0.8907210929064363\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2666 - custom_metric: 1.3542\n",
      "Epoch 11/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2498val_aucs: 0.47211426048569977 0.8880686352753391\n",
      "343/343 [==============================] - 16s 46ms/step - loss: 0.2494 - custom_metric: 1.3602\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2294val_aucs: 0.4817400638086589 0.8939075630252101\n",
      "343/343 [==============================] - 16s 47ms/step - loss: 0.2294 - custom_metric: 1.3756\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2058val_aucs: 0.4300057973464769 0.8857007886953664\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2058 - custom_metric: 1.3157\n",
      "Test res 0.8827212482048827 0.4700282815223404 0.4741966893865628\n",
      "Repeat 6 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_30ld.h5\n",
      "Epoch 1/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4895val_aucs: 0.4935498865188656 0.9006143884892087\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4895 - custom_metric: 1.3942\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4283val_aucs: 0.5308253411837254 0.9101899280575538\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4280 - custom_metric: 1.4410\n",
      "Epoch 3/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3995val_aucs: 0.5366305889745999 0.9090791366906475\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3995 - custom_metric: 1.4457\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3773val_aucs: 0.4980532129867594 0.8987856115107914\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3773 - custom_metric: 1.3968\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3658val_aucs: 0.5236866180448426 0.9101776978417266\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3658 - custom_metric: 1.4339\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3438val_aucs: 0.508132909196327 0.8996345323741007\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3438 - custom_metric: 1.4078\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3243val_aucs: 0.5230173869158742 0.8985410071942448\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3243 - custom_metric: 1.4216\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3111val_aucs: 0.4810545531607556 0.883746762589928\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3111 - custom_metric: 1.3648\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2852val_aucs: 0.4974077095635354 0.8936143884892086\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2852 - custom_metric: 1.3910\n",
      "Epoch 10/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2636val_aucs: 0.4673813355758194 0.8831999999999999\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2635 - custom_metric: 1.3506\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2408val_aucs: 0.47909586740567606 0.8823122302158274\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2408 - custom_metric: 1.3614\n",
      "Epoch 12/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.2312val_aucs: 0.47921561001459506 0.8890805755395683\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2309 - custom_metric: 1.3683\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1890val_aucs: 0.4369478334156242 0.8762043165467626\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1890 - custom_metric: 1.3132\n",
      "Test res 0.8876874588618957 0.4953829390699376 0.4951171875\n",
      "Repeat 7 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_30ld.h5\n",
      "Epoch 1/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4985val_aucs: 0.47892287875803524 0.885119830054927\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4985 - custom_metric: 1.3640\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4314val_aucs: 0.5096335725302497 0.8874524670203275\n",
      "343/343 [==============================] - 16s 46ms/step - loss: 0.4313 - custom_metric: 1.3971\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4014val_aucs: 0.5029476356547188 0.8903763320970847\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4014 - custom_metric: 1.3933\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3888val_aucs: 0.5298104586152793 0.8948098093986199\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3888 - custom_metric: 1.4246\n",
      "Epoch 5/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3612val_aucs: 0.5258829261173646 0.891623339279846\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3609 - custom_metric: 1.4175\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3403val_aucs: 0.5014785768090239 0.8813069222102249\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3403 - custom_metric: 1.3828\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3211val_aucs: 0.48258346171281946 0.8766504506830665\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3211 - custom_metric: 1.3592\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3022val_aucs: 0.47323602996315317 0.8834869841791466\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3022 - custom_metric: 1.3567\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2739val_aucs: 0.472683988559573 0.8781219191587251\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2739 - custom_metric: 1.3508\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2533val_aucs: 0.46354365491640825 0.8743852988122622\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2533 - custom_metric: 1.3379\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2256val_aucs: 0.4287813630139729 0.8667829561992395\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2256 - custom_metric: 1.2956\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2145val_aucs: 0.4646921752889474 0.867283226139618\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2145 - custom_metric: 1.3320\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2039val_aucs: 0.4237065189725385 0.8612887892587202\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2039 - custom_metric: 1.2850\n",
      "Epoch 14/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1846val_aucs: 0.4375661472224356 0.8618257358809446\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1846 - custom_metric: 1.2994\n",
      "Test res 0.881924664911441 0.4858101725014794 0.4833984375\n",
      "Repeat 8 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_30ld.h5\n",
      "Epoch 1/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4984val_aucs: 0.48772697990948266 0.8865179121493871\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4984 - custom_metric: 1.3742\n",
      "Epoch 2/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4264val_aucs: 0.4845794530100884 0.888380294157219\n",
      "343/343 [==============================] - 16s 45ms/step - loss: 0.4259 - custom_metric: 1.3730\n",
      "Epoch 3/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.4014val_aucs: 0.5487501965566965 0.8987402366570597\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.4019 - custom_metric: 1.4475\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3813val_aucs: 0.5457868652584306 0.8951981701919742\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3813 - custom_metric: 1.4410\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3545val_aucs: 0.5450877494768944 0.8866439592966852\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3545 - custom_metric: 1.4317\n",
      "Epoch 6/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3342val_aucs: 0.5603969300280446 0.8964189863826593\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3342 - custom_metric: 1.4568\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3222val_aucs: 0.5164872621038631 0.889455235559458\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3222 - custom_metric: 1.4059\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3051val_aucs: 0.5327747660576622 0.8819348945240304\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3051 - custom_metric: 1.4147\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2769val_aucs: 0.5313390984069107 0.8844360098288448\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2769 - custom_metric: 1.4158\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2419val_aucs: 0.5085547045652115 0.8758478086915881\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2419 - custom_metric: 1.3844\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2416val_aucs: 0.4931430409152988 0.8725705828618366\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2416 - custom_metric: 1.3657\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2085val_aucs: 0.48697008495983224 0.87190069184305\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2085 - custom_metric: 1.3589\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2060val_aucs: 0.49572067485609916 0.8769808167571893\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2060 - custom_metric: 1.3727\n",
      "Epoch 14/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1944val_aucs: 0.4454929849987351 0.8629272679634887\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1944 - custom_metric: 1.3084\n",
      "Epoch 15/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1705val_aucs: 0.45643431432549064 0.856270845578082\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1705 - custom_metric: 1.3127\n",
      "Epoch 16/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1637val_aucs: 0.44682276190947323 0.861091794897215\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1637 - custom_metric: 1.3079\n",
      "Test res 0.8788412237464097 0.45915325342000907 0.48977604673807207\n",
      "Repeat 9 ld 30\n",
      "Num train: 10965 Num valid: 2778\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_30ld.h5\n",
      "Epoch 1/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4908val_aucs: 0.5175078359555136 0.8936093432663036\n",
      "343/343 [==============================] - 23s 50ms/step - loss: 0.4908 - custom_metric: 1.4111\n",
      "Epoch 2/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.4246val_aucs: 0.5280272286592389 0.8993067033114552\n",
      "343/343 [==============================] - 16s 46ms/step - loss: 0.4246 - custom_metric: 1.4273\n",
      "Epoch 3/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3987val_aucs: 0.5374509088407259 0.8985431098318796\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3987 - custom_metric: 1.4360\n",
      "Epoch 4/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3762val_aucs: 0.547893162658747 0.9016090314776616\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3762 - custom_metric: 1.4495\n",
      "Epoch 5/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3595val_aucs: 0.5644028881776172 0.9039936372021588\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3595 - custom_metric: 1.4684\n",
      "Epoch 6/1000\n",
      "342/343 [============================>.] - ETA: 0s - loss: 0.3307val_aucs: 0.5283528768976244 0.8964977186020899\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3306 - custom_metric: 1.4249\n",
      "Epoch 7/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3173val_aucs: 0.5104128771353228 0.890807575886613\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3173 - custom_metric: 1.4012\n",
      "Epoch 8/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3079val_aucs: 0.4368314422497357 0.8700577530720564\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.3079 - custom_metric: 1.3069\n",
      "Epoch 9/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2775val_aucs: 0.5393529103526186 0.8896253772859087\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2775 - custom_metric: 1.4290\n",
      "Epoch 10/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2555val_aucs: 0.4749954943231462 0.8757623304468826\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2555 - custom_metric: 1.3508\n",
      "Epoch 11/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2370val_aucs: 0.48820631514352253 0.8826952973323307\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2370 - custom_metric: 1.3709\n",
      "Epoch 12/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2205val_aucs: 0.4235569962008538 0.8715575141784442\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.2205 - custom_metric: 1.2951\n",
      "Epoch 13/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1999val_aucs: 0.4150476410463128 0.8677611987695898\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1999 - custom_metric: 1.2828\n",
      "Epoch 14/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1834val_aucs: 0.4896551431285156 0.8825754896597316\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1834 - custom_metric: 1.3722\n",
      "Epoch 15/1000\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1720val_aucs: 0.4405085639791373 0.8717408343521797\n",
      "343/343 [==============================] - 15s 45ms/step - loss: 0.1720 - custom_metric: 1.3122\n",
      "Test res 0.8837980380864049 0.4768054483558192 0.49707602339181284\n",
      "gen_res {10: [(0.8611788638029558, 0.005761069640297871), (0.4258220233226783, 0.012674895180699959), (0.4489951723814096, 0.010112711805031717)], 20: [(0.8746336044085087, 0.00481443321810223), (0.4575831724601528, 0.015097915769981042), (0.47199052891003135, 0.008642564577024284)], 30: [(0.882907061542604, 0.003527873116855481), (0.47691505008510093, 0.01280622694736427), (0.48580599814652903, 0.007400472119190213)]}\n",
      "Repeat 0 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4978val_aucs: 0.45385611165069195 0.878743245000578\n",
      "457/457 [==============================] - 29s 50ms/step - loss: 0.4978 - custom_metric: 1.3326\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4229val_aucs: 0.4628108781948965 0.888829687319385\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4230 - custom_metric: 1.3516\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3997val_aucs: 0.47931742473123695 0.8902104886718298\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3997 - custom_metric: 1.3695\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3836val_aucs: 0.46248880050983554 0.8796056814241129\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3836 - custom_metric: 1.3421\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3646val_aucs: 0.48629482576294175 0.8913664244018032\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3646 - custom_metric: 1.3777\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3453val_aucs: 0.4607244064341967 0.876966896890533\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3453 - custom_metric: 1.3377\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3354val_aucs: 0.44231266097138067 0.8737365983701305\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3354 - custom_metric: 1.3160\n",
      "Epoch 8/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3063val_aucs: 0.4255932014378898 0.8630938619812738\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3065 - custom_metric: 1.2887\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2895val_aucs: 0.4358538246709932 0.8630830250838054\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2895 - custom_metric: 1.2989\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2656val_aucs: 0.43445628794727564 0.8632844107617617\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2656 - custom_metric: 1.2977\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2579val_aucs: 0.41443453343543424 0.8481687449427812\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2579 - custom_metric: 1.2626\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2429val_aucs: 0.42378638056711554 0.8555829889608139\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2429 - custom_metric: 1.2794\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2195val_aucs: 0.40885106781712505 0.8518397439602358\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2195 - custom_metric: 1.2607\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1926val_aucs: 0.41690582352101596 0.8470444168304242\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.1926 - custom_metric: 1.2640\n",
      "Epoch 15/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.1900val_aucs: 0.417239715644417 0.8533207866142642\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.1905 - custom_metric: 1.2706\n",
      "Test res 0.8904595126256581 0.48916163508505767 0.5009765625\n",
      "Repeat 1 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4705val_aucs: 0.4345611426967946 0.8821112602614709\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4705 - custom_metric: 1.3167\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4167val_aucs: 0.4533399372566043 0.8806687843064286\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4161 - custom_metric: 1.3340\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4002val_aucs: 0.47769950099907893 0.8819694935828426\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4002 - custom_metric: 1.3597\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3813val_aucs: 0.4850382154357461 0.877876866779194\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3813 - custom_metric: 1.3629\n",
      "Epoch 5/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3635val_aucs: 0.4872864461274304 0.8799005861166119\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3632 - custom_metric: 1.3672\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3448val_aucs: 0.47759557093583166 0.8772646119358684\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3448 - custom_metric: 1.3549\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3283val_aucs: 0.46315273396378565 0.8715496427036679\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3283 - custom_metric: 1.3347\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3139val_aucs: 0.4678757413638227 0.8718141261634836\n",
      "457/457 [==============================] - 21s 47ms/step - loss: 0.3139 - custom_metric: 1.3397\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2980val_aucs: 0.4457276846299416 0.8703818396884677\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2980 - custom_metric: 1.3161\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2709val_aucs: 0.43959644912211887 0.8628035246740474\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2709 - custom_metric: 1.3024\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2603val_aucs: 0.46200077675777446 0.8734692521364682\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2603 - custom_metric: 1.3355\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2323val_aucs: 0.4328681834768562 0.8639225953934689\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2323 - custom_metric: 1.2968\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2172val_aucs: 0.4268568620494823 0.860255268625705\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2172 - custom_metric: 1.2871\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1972val_aucs: 0.4009919765014565 0.8527504950758231\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.1972 - custom_metric: 1.2537\n",
      "Epoch 15/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1828val_aucs: 0.42583534439213555 0.8516642079008343\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.1828 - custom_metric: 1.2775\n",
      "Test res 0.8889532970320727 0.48797210327220375 0.4936708860759494\n",
      "Repeat 2 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4717val_aucs: 0.4697724603648964 0.8798070671020691\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4717 - custom_metric: 1.3496\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4144val_aucs: 0.5014342605699793 0.8918215740954805\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4151 - custom_metric: 1.3933\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3956val_aucs: 0.48773534431176463 0.8888044012252918\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3956 - custom_metric: 1.3765\n",
      "Epoch 4/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3725val_aucs: 0.4782788417055801 0.8844091362270258\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3725 - custom_metric: 1.3627\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3611val_aucs: 0.48952650236357387 0.88346271384811\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3611 - custom_metric: 1.3730\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3416val_aucs: 0.4526482626781472 0.8709506126459368\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3416 - custom_metric: 1.3236\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3232val_aucs: 0.4846514815682732 0.8801276586521789\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3232 - custom_metric: 1.3648\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3093val_aucs: 0.4671138189327157 0.8755969324355566\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3093 - custom_metric: 1.3427\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2893val_aucs: 0.462902157791873 0.8758290226563403\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2893 - custom_metric: 1.3387\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2705val_aucs: 0.45560779856750633 0.8715818619234772\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2705 - custom_metric: 1.3272\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2569val_aucs: 0.462747393739813 0.8697034663622704\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2569 - custom_metric: 1.3325\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2320val_aucs: 0.4309914769938434 0.8647817087619929\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2320 - custom_metric: 1.2958\n",
      "Test res 0.8877172839875538 0.48459921052162236 0.492725509214355\n",
      "Repeat 3 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4739val_aucs: 0.4807046246630828 0.8938711055308904\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4744 - custom_metric: 1.3746\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4129val_aucs: 0.5291927507186724 0.9015588662947477\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4129 - custom_metric: 1.4308\n",
      "Epoch 3/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3942val_aucs: 0.52236791044259 0.8945856038934983\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3940 - custom_metric: 1.4170\n",
      "Epoch 4/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3785val_aucs: 0.5385875581899756 0.901188673302527\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3784 - custom_metric: 1.4398\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3597val_aucs: 0.5132377609382518 0.8945510870643869\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3597 - custom_metric: 1.4078\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3429val_aucs: 0.5353261496985614 0.896087948880576\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3429 - custom_metric: 1.4314\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3278val_aucs: 0.5001135617165318 0.8903693732175294\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3278 - custom_metric: 1.3905\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3111val_aucs: 0.49431706190789126 0.889384780667124\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3111 - custom_metric: 1.3837\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2904val_aucs: 0.5014491436957316 0.8895711715443261\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2904 - custom_metric: 1.3910\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2710val_aucs: 0.4727789754469658 0.8763805652993688\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2710 - custom_metric: 1.3492\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2519val_aucs: 0.5044358389960858 0.8825383676128593\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2519 - custom_metric: 1.3870\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2461val_aucs: 0.5056881256261394 0.8805484724145817\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2461 - custom_metric: 1.3862\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2132val_aucs: 0.499909444265984 0.8802637085744118\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2132 - custom_metric: 1.3802\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2052val_aucs: 0.49045461013956976 0.8819791949812531\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2052 - custom_metric: 1.3724\n",
      "Test res 0.8925221210806606 0.4962799456099532 0.5\n",
      "Repeat 4 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4665val_aucs: 0.5184683426469573 0.8859414790454507\n",
      "457/457 [==============================] - 28s 50ms/step - loss: 0.4665 - custom_metric: 1.4044\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4111val_aucs: 0.5359255991128181 0.8960553166371532\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4111 - custom_metric: 1.4320\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3904val_aucs: 0.5367084528853734 0.8997149844000337\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3904 - custom_metric: 1.4364\n",
      "Epoch 4/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3717val_aucs: 0.5354046525323958 0.8947095033308035\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3716 - custom_metric: 1.4301\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3587val_aucs: 0.530918568673296 0.8917025044270174\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3587 - custom_metric: 1.4226\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3402val_aucs: 0.5246670171296847 0.8959684627708913\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3402 - custom_metric: 1.4206\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3235val_aucs: 0.4796294761139544 0.8761455434690952\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3235 - custom_metric: 1.3558\n",
      "Epoch 8/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3012val_aucs: 0.5000185534784074 0.8944337633864574\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3013 - custom_metric: 1.3945\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2875val_aucs: 0.49393805097506144 0.8848857407875876\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2875 - custom_metric: 1.3788\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2805val_aucs: 0.47876616637348535 0.8820347415465047\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2805 - custom_metric: 1.3608\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2418val_aucs: 0.45487209331276823 0.876908676954212\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2418 - custom_metric: 1.3318\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2438val_aucs: 0.4839847918081214 0.8865814992832448\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2438 - custom_metric: 1.3706\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2169val_aucs: 0.4421376298130914 0.8685917868285691\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2169 - custom_metric: 1.3107\n",
      "Test res 0.8881617625059837 0.4921381034968004 0.4932170542635659\n",
      "Repeat 5 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.5012val_aucs: 0.5365480481265655 0.8963082743674595\n",
      "457/457 [==============================] - 28s 49ms/step - loss: 0.5014 - custom_metric: 1.4329\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4226val_aucs: 0.5545059357674619 0.9034843572079728\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4226 - custom_metric: 1.4580\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4057val_aucs: 0.5560007533922907 0.9031187075704502\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4057 - custom_metric: 1.4591\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3843val_aucs: 0.5372255308564733 0.8975855436492167\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3843 - custom_metric: 1.4348\n",
      "Epoch 5/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3663val_aucs: 0.5294645022162131 0.8992025832562519\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3663 - custom_metric: 1.4287\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3517val_aucs: 0.5488908018310473 0.9016728053508399\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3517 - custom_metric: 1.4506\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3342val_aucs: 0.5415117236062829 0.9001667963415001\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3342 - custom_metric: 1.4417\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3146val_aucs: 0.5497476205560107 0.8966530535918816\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3146 - custom_metric: 1.4464\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2964val_aucs: 0.5192832211354605 0.8945292803547636\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2964 - custom_metric: 1.4138\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2719val_aucs: 0.5403860770522814 0.8944875395285624\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2719 - custom_metric: 1.4349\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2489val_aucs: 0.5079634601074551 0.8876387047654667\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2489 - custom_metric: 1.3956\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2324val_aucs: 0.49765144061232014 0.8884985657852117\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2324 - custom_metric: 1.3862\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2157val_aucs: 0.4946519538663149 0.8855458197397376\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2157 - custom_metric: 1.3802\n",
      "Test res 0.8865246061961466 0.47067593166451216 0.48927875243664715\n",
      "Repeat 6 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4762val_aucs: 0.5476110283784243 0.9062779656304624\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4762 - custom_metric: 1.4539\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4271val_aucs: 0.5784219331058652 0.9113677597402341\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4271 - custom_metric: 1.4898\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3951val_aucs: 0.5775517078795593 0.908467035210728\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3951 - custom_metric: 1.4860\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3839val_aucs: 0.5934033176176212 0.9123795577313293\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3839 - custom_metric: 1.5058\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3651val_aucs: 0.5468847408727459 0.89790792401326\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3651 - custom_metric: 1.4448\n",
      "Epoch 6/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3522val_aucs: 0.5724401736252107 0.9067747813497432\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3528 - custom_metric: 1.4792\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3351val_aucs: 0.5538934739496758 0.9026020032178494\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3351 - custom_metric: 1.4565\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3200val_aucs: 0.5714608196030767 0.9062116182212102\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3200 - custom_metric: 1.4777\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3023val_aucs: 0.5619817431467555 0.8993636177662566\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3023 - custom_metric: 1.4613\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2795val_aucs: 0.5543913891295981 0.9030103557247939\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2795 - custom_metric: 1.4574\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2585val_aucs: 0.5377274735897752 0.8900236402137967\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2585 - custom_metric: 1.4278\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2381val_aucs: 0.528616450852956 0.888736974384371\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2381 - custom_metric: 1.4174\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2220val_aucs: 0.5169592306244077 0.8893964992263418\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2220 - custom_metric: 1.4064\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2096val_aucs: 0.49528270058581747 0.8910346483547817\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2096 - custom_metric: 1.3863\n",
      "Test res 0.8844728899892293 0.47068398996508304 0.478515625\n",
      "Repeat 7 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4772val_aucs: 0.5209683886935881 0.9011170850107775\n",
      "457/457 [==============================] - 29s 48ms/step - loss: 0.4766 - custom_metric: 1.4221\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4253val_aucs: 0.5741377630668747 0.9032583686593795\n",
      "457/457 [==============================] - 21s 46ms/step - loss: 0.4252 - custom_metric: 1.4774\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4065val_aucs: 0.5906658217499965 0.9105103774456722\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4065 - custom_metric: 1.5012\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3863val_aucs: 0.5768694385297828 0.9102248466308549\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3863 - custom_metric: 1.4871\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3729val_aucs: 0.5699379758135017 0.9111181671054284\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3729 - custom_metric: 1.4811\n",
      "Epoch 6/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3586val_aucs: 0.5621846710285442 0.9071586485032735\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3584 - custom_metric: 1.4693\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3424val_aucs: 0.5542139323763045 0.9019408986756583\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3424 - custom_metric: 1.4562\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3274val_aucs: 0.5533782168168797 0.9035632508018954\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3274 - custom_metric: 1.4569\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3145val_aucs: 0.557858848258313 0.9034503022361446\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3145 - custom_metric: 1.4613\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2941val_aucs: 0.5264951155441915 0.8994141681733057\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2941 - custom_metric: 1.4259\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2738val_aucs: 0.5071994203687099 0.886605958787201\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2738 - custom_metric: 1.3938\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2614val_aucs: 0.49396166606841607 0.8916736370938887\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2614 - custom_metric: 1.3856\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2417val_aucs: 0.5020106958112995 0.8854425095749581\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2417 - custom_metric: 1.3875\n",
      "Test res 0.8851747621469602 0.4930455366460836 0.49951409135082603\n",
      "Repeat 8 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4815val_aucs: 0.5656454919124592 0.9019345369546161\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4815 - custom_metric: 1.4676\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4187val_aucs: 0.5442171504344551 0.9016453718866624\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4187 - custom_metric: 1.4459\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4033val_aucs: 0.5727825013913092 0.9041353624718103\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.4033 - custom_metric: 1.4769\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3844val_aucs: 0.5769979663429832 0.9053743177629646\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3844 - custom_metric: 1.4824\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3719val_aucs: 0.55200153296442 0.9050662776905753\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3719 - custom_metric: 1.4571\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3574val_aucs: 0.5457572477954898 0.8962463656178961\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3574 - custom_metric: 1.4420\n",
      "Epoch 7/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3402val_aucs: 0.5789970866890792 0.8977276759660039\n",
      "457/457 [==============================] - 21s 46ms/step - loss: 0.3402 - custom_metric: 1.4767\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3263val_aucs: 0.5434631185144089 0.891480804497989\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3263 - custom_metric: 1.4349\n",
      "Epoch 9/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3080val_aucs: 0.5280990370145534 0.8893188614899324\n",
      "457/457 [==============================] - 21s 46ms/step - loss: 0.3083 - custom_metric: 1.4174\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2890val_aucs: 0.5484649349158485 0.8899036091273481\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2890 - custom_metric: 1.4384\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2676val_aucs: 0.5264882379156507 0.8856839131357195\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2676 - custom_metric: 1.4122\n",
      "Epoch 12/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2548val_aucs: 0.5506248064818708 0.8924011497142702\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2552 - custom_metric: 1.4430\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2408val_aucs: 0.5467809603992235 0.8885363038060313\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2408 - custom_metric: 1.4353\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2149val_aucs: 0.5236442682797534 0.8898877541236221\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2149 - custom_metric: 1.4135\n",
      "Test res 0.8866311445368596 0.48929754949388415 0.49073170731707316\n",
      "Repeat 9 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4751val_aucs: 0.49597487908696986 0.8861434979278756\n",
      "457/457 [==============================] - 29s 49ms/step - loss: 0.4751 - custom_metric: 1.3821\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4258val_aucs: 0.5252390890622517 0.8901990225184748\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.4256 - custom_metric: 1.4154\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3935val_aucs: 0.5515028818203167 0.8886725133560074\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3935 - custom_metric: 1.4402\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3829val_aucs: 0.557695245589477 0.8989728481567003\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3829 - custom_metric: 1.4567\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3627val_aucs: 0.5370768960904186 0.8819916660031317\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3627 - custom_metric: 1.4191\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3500val_aucs: 0.5430733394026765 0.8930488096186002\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3500 - custom_metric: 1.4361\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3350val_aucs: 0.5341516932545551 0.8936319619017286\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.3350 - custom_metric: 1.4278\n",
      "Epoch 8/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3214val_aucs: 0.5274449211288007 0.8904841528935805\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.3211 - custom_metric: 1.4179\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2991val_aucs: 0.5383565761673734 0.8918445887790579\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2991 - custom_metric: 1.4302\n",
      "Epoch 10/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2787val_aucs: 0.5115465376919633 0.8822100637372553\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2790 - custom_metric: 1.3938\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2636val_aucs: 0.5201094619144043 0.8865909099526426\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2636 - custom_metric: 1.4067\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2426val_aucs: 0.5120131510206567 0.8790614964036415\n",
      "457/457 [==============================] - 21s 45ms/step - loss: 0.2426 - custom_metric: 1.3911\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2366val_aucs: 0.5017279152067424 0.877214594731913\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2366 - custom_metric: 1.3789\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2156val_aucs: 0.46340821879728855 0.8715290495527777\n",
      "457/457 [==============================] - 20s 45ms/step - loss: 0.2156 - custom_metric: 1.3349\n",
      "Test res 0.8826403744315463 0.4807073091296994 0.48446601941747575\n",
      "gen_res {10: [(0.8611788638029558, 0.005761069640297871), (0.4258220233226783, 0.012674895180699959), (0.4489951723814096, 0.010112711805031717)], 20: [(0.8746336044085087, 0.00481443321810223), (0.4575831724601528, 0.015097915769981042), (0.47199052891003135, 0.008642564577024284)], 30: [(0.882907061542604, 0.003527873116855481), (0.47691505008510093, 0.01280622694736427), (0.48580599814652903, 0.007400472119190213)], 40: [(0.887325775453267, 0.002759827477523149), (0.48545613148849, 0.008450894835920101), (0.4923096207575893, 0.006726693282573095)]}\n",
      "Repeat 0 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4645val_aucs: 0.5049042177056993 0.8856156723211452\n",
      "572/572 [==============================] - 34s 49ms/step - loss: 0.4645 - custom_metric: 1.3905\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4156val_aucs: 0.5075608103526003 0.8877266083012114\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4156 - custom_metric: 1.3953\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3977val_aucs: 0.5555074595549626 0.8915812252584051\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3977 - custom_metric: 1.4471\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3810val_aucs: 0.5444342608746016 0.8924686714836375\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3810 - custom_metric: 1.4369\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3630val_aucs: 0.5233564976494324 0.8913013623808111\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3630 - custom_metric: 1.4147\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3522val_aucs: 0.5255753255737993 0.88595015527708\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3522 - custom_metric: 1.4115\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3325val_aucs: 0.5279031767535629 0.8818671743732996\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3325 - custom_metric: 1.4098\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3166val_aucs: 0.5108002724568257 0.8834563585574058\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3166 - custom_metric: 1.3943\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2997val_aucs: 0.5163880873714598 0.8825304181817235\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2997 - custom_metric: 1.3989\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2807val_aucs: 0.5102956968350078 0.8781264792937884\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2807 - custom_metric: 1.3884\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2613val_aucs: 0.48203279560855283 0.8677216144654775\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2613 - custom_metric: 1.3498\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2461val_aucs: 0.467866949004516 0.8683302381955608\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2461 - custom_metric: 1.3362\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2363val_aucs: 0.5138454298908604 0.8735378725219392\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2363 - custom_metric: 1.3874\n",
      "Test res 0.8881248784556007 0.4880548847596777 0.505859375\n",
      "Repeat 1 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4675val_aucs: 0.5169575059197348 0.8960142443328283\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4675 - custom_metric: 1.4130\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4139val_aucs: 0.5385311659571989 0.8996949161550932\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4139 - custom_metric: 1.4382\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3955val_aucs: 0.5291312878401006 0.8996731244518856\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3955 - custom_metric: 1.4288\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3790val_aucs: 0.5362615147624059 0.8997446118685059\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3790 - custom_metric: 1.4360\n",
      "Epoch 5/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3612val_aucs: 0.504910573523899 0.8981184724547557\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3612 - custom_metric: 1.4030\n",
      "Epoch 6/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3490val_aucs: 0.5184232125743562 0.8961253288686918\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3489 - custom_metric: 1.4145\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3335val_aucs: 0.48094060071343225 0.8876876876876876\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3335 - custom_metric: 1.3686\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3137val_aucs: 0.4933511951675219 0.8887379414813042\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3137 - custom_metric: 1.3821\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3000val_aucs: 0.4733855285202863 0.8898588854341066\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3000 - custom_metric: 1.3632\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2814val_aucs: 0.49803267335584256 0.8858476175290335\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2814 - custom_metric: 1.3839\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2607val_aucs: 0.4757543747211409 0.8824481118286428\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2607 - custom_metric: 1.3582\n",
      "Epoch 12/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2455val_aucs: 0.46934622788363173 0.8814446304711792\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2455 - custom_metric: 1.3508\n",
      "Test res 0.8897255714456679 0.4893208937122421 0.4990234375\n",
      "Repeat 2 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4651val_aucs: 0.5384861775336967 0.8962806641929459\n",
      "572/572 [==============================] - 34s 50ms/step - loss: 0.4651 - custom_metric: 1.4348\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4185val_aucs: 0.5457408650705626 0.9020355246975388\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4185 - custom_metric: 1.4478\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3950val_aucs: 0.5424707348100756 0.9032195240097343\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3950 - custom_metric: 1.4457\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3817val_aucs: 0.5414917630988492 0.9043862233541178\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3817 - custom_metric: 1.4459\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3646val_aucs: 0.52652479913011 0.8925493756808588\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3646 - custom_metric: 1.4191\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3536val_aucs: 0.5339444460609336 0.8992486571555289\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3536 - custom_metric: 1.4332\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3400val_aucs: 0.5163169042533973 0.8970295431026077\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3400 - custom_metric: 1.4133\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3191val_aucs: 0.5164811348571916 0.8991380422098245\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3191 - custom_metric: 1.4156\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3080val_aucs: 0.5131680175483354 0.895721036446314\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3080 - custom_metric: 1.4089\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2895val_aucs: 0.49097572754445346 0.8939983790454402\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2895 - custom_metric: 1.3850\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2987val_aucs: 0.518505422344779 0.8997010775258738\n",
      "572/572 [==============================] - 27s 46ms/step - loss: 0.2987 - custom_metric: 1.4182\n",
      "Epoch 12/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2588val_aucs: 0.5063869492181404 0.8948764834722398\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2587 - custom_metric: 1.4013\n",
      "Test res 0.8892656661381043 0.49136686392756046 0.49560975609756097\n",
      "Repeat 3 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4724val_aucs: 0.5082523543849302 0.8946650558382568\n",
      "572/572 [==============================] - 33s 48ms/step - loss: 0.4724 - custom_metric: 1.4029\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4103val_aucs: 0.5347744563615199 0.8994303046344045\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4103 - custom_metric: 1.4342\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3944val_aucs: 0.4979120590791884 0.9024265948908847\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3944 - custom_metric: 1.4003\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3727val_aucs: 0.49488943577641 0.9003369292373895\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3727 - custom_metric: 1.3952\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3632val_aucs: 0.5513364896488454 0.9082948413647075\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3632 - custom_metric: 1.4596\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3515val_aucs: 0.568666503191798 0.9066614229363751\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3515 - custom_metric: 1.4753\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3335val_aucs: 0.54540290637572 0.905802290734606\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3335 - custom_metric: 1.4512\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3177val_aucs: 0.5110392571594099 0.9019927598101158\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3177 - custom_metric: 1.4130\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3000val_aucs: 0.5354240273485612 0.9048252279635258\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3000 - custom_metric: 1.4402\n",
      "Epoch 10/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2826val_aucs: 0.5293948987580924 0.9044943820224719\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2826 - custom_metric: 1.4339\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2677val_aucs: 0.4992575420537572 0.8927888391789899\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2677 - custom_metric: 1.3920\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2510val_aucs: 0.5130033103132656 0.9000861266691713\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2510 - custom_metric: 1.4131\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2317val_aucs: 0.49572084100262076 0.8925679194699634\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2317 - custom_metric: 1.3883\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2201val_aucs: 0.5067613097636654 0.8938149952187426\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2201 - custom_metric: 1.4006\n",
      "Epoch 15/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2018val_aucs: 0.4866845758698329 0.8941015504934942\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2018 - custom_metric: 1.3808\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1846val_aucs: 0.4446286290650322 0.8827583373177145\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.1846 - custom_metric: 1.3274\n",
      "Test res 0.8931944485998085 0.5031347551667552 0.5087890625\n",
      "Repeat 4 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4676val_aucs: 0.4992516398595188 0.8940224941520867\n",
      "572/572 [==============================] - 35s 48ms/step - loss: 0.4676 - custom_metric: 1.3933\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4132val_aucs: 0.528383292245833 0.9007962179124915\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4132 - custom_metric: 1.4292\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3936val_aucs: 0.5380726851046556 0.9081046679507088\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3936 - custom_metric: 1.4462\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3742val_aucs: 0.4936664584687571 0.9012276686248832\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3742 - custom_metric: 1.3949\n",
      "Epoch 5/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3608val_aucs: 0.5261263570952731 0.904643625905496\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3607 - custom_metric: 1.4308\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3486val_aucs: 0.5188517533410998 0.9027222809955135\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3486 - custom_metric: 1.4216\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3303val_aucs: 0.5115784847076996 0.8914579049843829\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3303 - custom_metric: 1.4030\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3134val_aucs: 0.5263496652802263 0.898224813605953\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3134 - custom_metric: 1.4246\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3005val_aucs: 0.5204705644358057 0.8934704679064629\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3005 - custom_metric: 1.4139\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2812val_aucs: 0.5161854059711102 0.890213880026296\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2812 - custom_metric: 1.4064\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2637val_aucs: 0.5090476471640804 0.8935454344336461\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2637 - custom_metric: 1.4026\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2468val_aucs: 0.48224631482566144 0.8870222980857848\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2468 - custom_metric: 1.3693\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2361val_aucs: 0.5125483566510192 0.886016802986918\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2361 - custom_metric: 1.3986\n",
      "Test res 0.8969770972953567 0.5151603040701617 0.5285024154589372\n",
      "Repeat 5 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4738val_aucs: 0.5009409150188443 0.8969563053395427\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4738 - custom_metric: 1.3979\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4153val_aucs: 0.5327745647389873 0.9027553663307327\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4153 - custom_metric: 1.4355\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3959val_aucs: 0.5320502069936706 0.8971887855760807\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3959 - custom_metric: 1.4292\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3817val_aucs: 0.529535730329383 0.9019752366278952\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3817 - custom_metric: 1.4315\n",
      "Epoch 5/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3702val_aucs: 0.5359153343360372 0.8991130878976074\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3701 - custom_metric: 1.4350\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3548val_aucs: 0.5389714709412703 0.9036490943309695\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3548 - custom_metric: 1.4426\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3388val_aucs: 0.5372191587444053 0.9025017515272366\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3388 - custom_metric: 1.4397\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3230val_aucs: 0.5076262181719089 0.897069903636942\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3230 - custom_metric: 1.4047\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3161val_aucs: 0.5354186968597656 0.9000615015898478\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3161 - custom_metric: 1.4355\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2894val_aucs: 0.5234200629136793 0.8959481864956458\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2894 - custom_metric: 1.4194\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2743val_aucs: 0.5149575885026173 0.8949305570966176\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2743 - custom_metric: 1.4099\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2548val_aucs: 0.5092032347161206 0.8941697126861295\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2548 - custom_metric: 1.4034\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2361val_aucs: 0.5125222848822544 0.8918100382852678\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2361 - custom_metric: 1.4043\n",
      "Epoch 14/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2295val_aucs: 0.5212808205625507 0.8912536708100984\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2295 - custom_metric: 1.4125\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2093val_aucs: 0.46989953390873934 0.8826814904519309\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2093 - custom_metric: 1.3526\n",
      "Epoch 16/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.1961val_aucs: 0.49982789095318175 0.8902540057929849\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.1961 - custom_metric: 1.3901\n",
      "Test res 0.8871303180349449 0.5040685373434545 0.5078125\n",
      "Repeat 6 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4696val_aucs: 0.5276663112519936 0.8932160751126268\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4696 - custom_metric: 1.4209\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4113val_aucs: 0.5397767394027059 0.9017420319144458\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4113 - custom_metric: 1.4415\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3927val_aucs: 0.5253830554864666 0.9003173550587344\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3927 - custom_metric: 1.4257\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3754val_aucs: 0.5384804998688678 0.9018936044798114\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3754 - custom_metric: 1.4404\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3642val_aucs: 0.5318829248301188 0.8982884931160794\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3642 - custom_metric: 1.4302\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3478val_aucs: 0.5127452878063077 0.8967043492905561\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3478 - custom_metric: 1.4094\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3344val_aucs: 0.5311243038517319 0.9017436107953349\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3344 - custom_metric: 1.4329\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3184val_aucs: 0.5083031923081663 0.8953575638920467\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3184 - custom_metric: 1.4037\n",
      "Epoch 9/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3001val_aucs: 0.5187008384015418 0.8962696307523895\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3002 - custom_metric: 1.4150\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2912val_aucs: 0.4940155361555402 0.8945975958906993\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2912 - custom_metric: 1.3886\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2689val_aucs: 0.5065541974766654 0.8937918403435645\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2689 - custom_metric: 1.4003\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2463val_aucs: 0.47374676868694915 0.887222643257126\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2463 - custom_metric: 1.3610\n",
      "Test res 0.8905107015168742 0.49996432221768633 0.5029296875\n",
      "Repeat 7 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4724val_aucs: 0.48576197145131594 0.8872243931643108\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4724 - custom_metric: 1.3730\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4120val_aucs: 0.5243934332601438 0.9013026351520668\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4120 - custom_metric: 1.4257\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3946val_aucs: 0.529855184274336 0.9014373652044173\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3946 - custom_metric: 1.4313\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3849val_aucs: 0.5192731508624847 0.9007380271116707\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3849 - custom_metric: 1.4200\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3710val_aucs: 0.5250042795012724 0.8954615369291403\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3710 - custom_metric: 1.4205\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3573val_aucs: 0.5090261359217328 0.8906437580143411\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3573 - custom_metric: 1.3997\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3352val_aucs: 0.5182992775223835 0.8945656082930802\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3352 - custom_metric: 1.4129\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3284val_aucs: 0.5047761168713719 0.8926903966305954\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3284 - custom_metric: 1.3975\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3109val_aucs: 0.5038639457761457 0.8853913829384571\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3109 - custom_metric: 1.3893\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2911val_aucs: 0.5006090409509848 0.8821541919918973\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2911 - custom_metric: 1.3828\n",
      "Epoch 11/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2792val_aucs: 0.5233642579655022 0.8868964801380642\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2792 - custom_metric: 1.4103\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2562val_aucs: 0.497006636688426 0.8784430867755901\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2562 - custom_metric: 1.3754\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2499val_aucs: 0.48727346109042724 0.8802165955970008\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2499 - custom_metric: 1.3675\n",
      "Test res 0.8895333443034946 0.5058508545594124 0.4975609756097561\n",
      "Repeat 8 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4680val_aucs: 0.48244598186572774 0.8856395090302221\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4680 - custom_metric: 1.3681\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4154val_aucs: 0.5315186105220302 0.904334292342393\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4154 - custom_metric: 1.4359\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3942val_aucs: 0.5127475939651754 0.9020498148951788\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3942 - custom_metric: 1.4148\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3813val_aucs: 0.5325103605907965 0.9002410916630503\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3813 - custom_metric: 1.4328\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3795val_aucs: 0.5162686863860212 0.8982802128035746\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3795 - custom_metric: 1.4145\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3511val_aucs: 0.5244474105717211 0.893911096109852\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.3511 - custom_metric: 1.4184\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3432val_aucs: 0.5263188328012636 0.8933115815077335\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3432 - custom_metric: 1.4196\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3277val_aucs: 0.5180819201888164 0.8937503683344853\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3277 - custom_metric: 1.4118\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3095val_aucs: 0.5077312719092989 0.8910292470975243\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3095 - custom_metric: 1.3988\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2942val_aucs: 0.48107637830394506 0.8840300882395487\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2942 - custom_metric: 1.3651\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2822val_aucs: 0.4890644960591555 0.8843017181799186\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2822 - custom_metric: 1.3734\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2676val_aucs: 0.4815079354388446 0.8854600296810625\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.2676 - custom_metric: 1.3670\n",
      "Test res 0.8875797518250358 0.4923153376926472 0.5150925024342746\n",
      "Repeat 9 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4747val_aucs: 0.5021641473642879 0.8911444285618806\n",
      "572/572 [==============================] - 34s 48ms/step - loss: 0.4747 - custom_metric: 1.3933\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4199val_aucs: 0.491302513663378 0.896158216208408\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4199 - custom_metric: 1.3875\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4028val_aucs: 0.5205578799020312 0.902753432897605\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4028 - custom_metric: 1.4233\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3863val_aucs: 0.5259360723476801 0.9067118565136119\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3863 - custom_metric: 1.4326\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3751val_aucs: 0.5272904930572131 0.9008906453136016\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3751 - custom_metric: 1.4282\n",
      "Epoch 6/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3646val_aucs: 0.5120777126066078 0.9033565750580503\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3646 - custom_metric: 1.4154\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3521val_aucs: 0.5177261845381063 0.9000929237787854\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3520 - custom_metric: 1.4178\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3358val_aucs: 0.5024916086588633 0.8973359334743359\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3358 - custom_metric: 1.3998\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3211val_aucs: 0.5239947834940276 0.8986494790015976\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.3211 - custom_metric: 1.4226\n",
      "Epoch 10/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3287val_aucs: 0.47304609946150944 0.8807206875066027\n",
      "572/572 [==============================] - 26s 46ms/step - loss: 0.3286 - custom_metric: 1.3538\n",
      "Epoch 11/1000\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.2943"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [10,20,30,40,50]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "f = open('log.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()\n",
    "\n",
    "# # save to local\n",
    "# log_path = '/content/log.csv'\n",
    "# files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "repeats[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 9 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4658val_aucs: 0.47376361522381083 0.8825373803579436\n",
      "572/572 [==============================] - 34s 49ms/step - loss: 0.4658 - custom_metric: 1.3563\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4129val_aucs: 0.5132164161627492 0.8931467037194909\n",
      "572/572 [==============================] - 26s 45ms/step - loss: 0.4129 - custom_metric: 1.4064\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3975val_aucs: 0.5097227826027497 0.898671966253882\n",
      "572/572 [==============================] - 25s 45ms/step - loss: 0.3975 - custom_metric: 1.4084\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3823val_aucs: 0.5000986501228106 0.8923639443543343\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3823 - custom_metric: 1.3925\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3679val_aucs: 0.5034327481245401 0.8936310845147611\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3679 - custom_metric: 1.3971\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3523val_aucs: 0.44784794543974393 0.8815032965572981\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3523 - custom_metric: 1.3294\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3399val_aucs: 0.4903898885056471 0.8874188316181642\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3399 - custom_metric: 1.3778\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3230val_aucs: 0.4498782287814017 0.8883720930232558\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3230 - custom_metric: 1.3383\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3108val_aucs: 0.49069287869230793 0.8913437000049822\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.3108 - custom_metric: 1.3820\n",
      "Epoch 10/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2906val_aucs: 0.47390006447910227 0.8887208471958502\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.2906 - custom_metric: 1.3626\n",
      "Epoch 11/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2746val_aucs: 0.44465136997722765 0.8827278112077412\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.2747 - custom_metric: 1.3274\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2500val_aucs: 0.4763349491913422 0.886837021085788\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.2500 - custom_metric: 1.3632\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2327val_aucs: 0.45057321286504864 0.875092862718179\n",
      "572/572 [==============================] - 25s 44ms/step - loss: 0.2327 - custom_metric: 1.3257\n",
      "Test res 0.8945075020943035 0.5106075352729654 0.51171875\n"
     ]
    }
   ],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [10,20,30,40,50]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "# f = open('log.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "\n",
    "ld = 50\n",
    "i = 9\n",
    "\n",
    "np.random.shuffle(train_inds)\n",
    "np.random.shuffle(valid_inds)\n",
    "train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "# f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "all_test_res = []\n",
    "print ('Repeat', i, 'ld', ld)\n",
    "# Get train and validation data.\n",
    "curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "curr_train_op = train_op[curr_train_ind]\n",
    "curr_valid_op = valid_op[curr_valid_ind]\n",
    "print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "# Construct save_path.\n",
    "savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "print (savepath)\n",
    "# Build and compile model.\n",
    "model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "# Load pretrained weights here.\n",
    "fore_model.load_weights(fore_savepath)\n",
    "# Train model.\n",
    "es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                   restore_best_weights=True)\n",
    "cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                verbose=1, callbacks=[cus, es]).history\n",
    "model.save_weights(savepath)\n",
    "# Test and write to log.\n",
    "rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "# f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "print ('Test res', rocauc, prauc, minrp)\n",
    "all_test_res.append([rocauc, prauc, minrp])\n",
    "\n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF-_1we4f-sb"
   },
   "source": [
    "#### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "IN4C7fvqf73n",
    "outputId": "67cf5bf6-1cdd-4745-e919-ea3c7a6ddee2"
   },
   "outputs": [],
   "source": [
    "x= range(10,51,10)\n",
    "for i,metric in enumerate(['ROC-AUC', 'PR-AUC', 'min(Re,Pr)']):\n",
    "    plt.figure()\n",
    "    y = [gen_res[ld][i][0] for ld in x]\n",
    "    plt.plot(x, y, color='r', marker='^')\n",
    "    plt.xlabel('% labeled data')\n",
    "    plt.ylabel(metric)\n",
    "    # save plt\n",
    "    plt.savefig(metric+'.png')    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
